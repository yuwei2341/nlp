{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic search with sentence embedding\n",
    "Search for the comment of github issues best matching a given query using text including the issue title, issue body text and the comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
       "    num_rows: 3019\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset = load_dataset(\"lewtun/github-issues\", split=\"train\")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_dataset = issues_dataset.filter(\n",
    "    lambda x: ((not x['is_pull_request']) and len(x['comments'])) > 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 808\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = issues_dataset.column_names\n",
    "columns_to_keep = [\"title\", \"body\", \"html_url\", \"comments\"]\n",
    "# same as minus here since set 1 is a subset of set 2\n",
    "columns_to_remove = set(columns_to_keep).symmetric_difference(columns)\n",
    "issues_dataset = issues_dataset.remove_columns(columns_to_remove)\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explode rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = issues_dataset.to_pandas()\n",
    "\n",
    "# comments_df = df.explode('comments', ignore_index=True)\n",
    "# comments_df.head()\n",
    "\n",
    "# comments_dataset = Dataset.from_pandas(comments_df)\n",
    "# comments_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use .map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ac30ab2e094f419a82aaede25a264e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/808 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 2964\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def map_to_explode(examples):\n",
    "    result = {k: [] for k in examples}\n",
    "    comments = examples.pop('comments')\n",
    "    for i, comment_i in enumerate(comments):\n",
    "        n_rows_to_explode = len(comment_i)\n",
    "        for k, v in examples.items():\n",
    "            result[k] += [v[i]] * n_rows_to_explode\n",
    "        result['comments'] += comment_i\n",
    "    return result\n",
    "\n",
    "comments_dataset = issues_dataset.map(map_to_explode, batched=True)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create some features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3782a4e45484665b15566eca7b1d19a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdcdd45bc4614f7c83c520be9ca9769a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],\n",
       "    num_rows: 2934\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset = comments_dataset.map(lambda x: {'comment_length': len(x['comments'].split())})\n",
    "comments_dataset = comments_dataset.filter(lambda x: x['comment_length'] > 1)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c05d59d1ec4482baa988ee4b7b5404f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2934 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'html_url': 'https://github.com/huggingface/datasets/issues/2945',\n",
       " 'title': 'Protect master branch',\n",
       " 'comments': 'Cool, I think we can do both :)',\n",
       " 'body': 'After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\\r\\n- 00cc036fea7c7745cfe722360036ed306796a3f2\\r\\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\\r\\n- ...\\r\\n\\r\\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\\r\\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\\r\\n  - Currently, simple merge commits are already disabled\\r\\n  - I propose to disable rebase merging as well\\r\\n- ~~Protect the master branch from direct pushes (to avoid accidentally pushing of merge commits)~~\\r\\n  - ~~This protection would reject direct pushes to master branch~~\\r\\n  - ~~If so, for each release (when we need to commit directly to the master branch), we should previously disable the protection and re-enable it again after the release~~\\r\\n- [x] Protect the master branch only from direct pushing of **merge commits**\\r\\n  - GitHub offers the possibility to protect the master branch only from merge commits (which are the ones that introduce all the commits from the feature branch into the master branch).\\r\\n  - No need to disable/re-enable this protection on each release \\r\\n\\r\\nThis purpose of this Issue is to open a discussion about this problem and to agree in a solution.',\n",
       " 'comment_length': 8,\n",
       " 'text': 'Protect master branch \\n After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\\r\\n- 00cc036fea7c7745cfe722360036ed306796a3f2\\r\\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\\r\\n- ...\\r\\n\\r\\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\\r\\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\\r\\n  - Currently, simple merge commits are already disabled\\r\\n  - I propose to disable rebase merging as well\\r\\n- ~~Protect the master branch from direct pushes (to avoid accidentally pushing of merge commits)~~\\r\\n  - ~~This protection would reject direct pushes to master branch~~\\r\\n  - ~~If so, for each release (when we need to commit directly to the master branch), we should previously disable the protection and re-enable it again after the release~~\\r\\n- [x] Protect the master branch only from direct pushing of **merge commits**\\r\\n  - GitHub offers the possibility to protect the master branch only from merge commits (which are the ones that introduce all the commits from the feature branch into the master branch).\\r\\n  - No need to disable/re-enable this protection on each release \\r\\n\\r\\nThis purpose of this Issue is to open a discussion about this problem and to agree in a solution. \\n Cool, I think we can do both :)'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset = comments_dataset.map(\n",
    "    lambda x: {'text': \" \\n \".join([x['title'], x['body'], x['comments']])}\n",
    ")\n",
    "comments_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings\n",
    "\n",
    "Instructions: https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Code\\env\\penv\\Lib\\site-packages\\transformers\\utils\\hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/new-impl:\n",
      "- configuration.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/new-impl:\n",
      "- modeling.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "d:\\Code\\env\\penv\\Lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "d:\\Code\\env\\penv\\Lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "A matching Triton is not available, some optimizations will not be enabled.\n",
      "Error caught was: No module named 'triton'\n",
      "Some weights of the model checkpoint at Alibaba-NLP/gte-multilingual-base were not used when initializing NewModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn.functional as F\n",
    "model_ckpt = 'Alibaba-NLP/gte-multilingual-base'  # \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8816621899604797, 0.3257666230201721, 0.7938992977142334, 0.3752664625644684]]\n"
     ]
    }
   ],
   "source": [
    "# Test the Ali GTE model\n",
    "if model_ckpt == 'Alibaba-NLP/gte-multilingual-base':\n",
    "    input_texts = [\n",
    "        \"what is the capital of China?\",\n",
    "        \"中国的首都是哪儿?\",\n",
    "        \"how to implement quick sort in python?\",\n",
    "        \"北京\",\n",
    "        \"快排算法介绍\",\n",
    "    ]\n",
    "\n",
    "    max_length = 8192  # max 8192\n",
    "    batch_dict = tokenizer(input_texts, max_length=max_length, padding=True, truncation=True, return_tensors='pt')\n",
    "    model.to('cpu')\n",
    "    outputs = model(**batch_dict)\n",
    "\n",
    "    dimension = 256 # Truncate the output dimension of the output embedding, should be in [128, 768]\n",
    "    embeddings = outputs.last_hidden_state[:, 0][:, :dimension]  \n",
    "\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    scores = (embeddings[:1] @ embeddings[1:].T)  # Matrix mul to check the similarity between the normalized embeddings of the first and the rest\n",
    "    print(scores.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NewModel(\n",
       "  (embeddings): NewEmbeddings(\n",
       "    (word_embeddings): Embedding(250048, 768, padding_idx=1)\n",
       "    (rotary_emb): NTKScalingRotaryEmbedding()\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): NewEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x NewLayer(\n",
       "        (attention): NewAttention(\n",
       "          (qkv_proj): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (mlp): NewGatedMLP(\n",
       "          (up_gate_proj): Linear(in_features=768, out_features=6144, bias=False)\n",
       "          (down_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act_fn): GELUActivation()\n",
       "          (hidden_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (mlp_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (hidden_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLS pooling\n",
    "Pooling is the process of converting a sequence of embeddings into a sentence embedding is called “pooling”.  \n",
    "One way is using CLS pooling: to collect the last hidden state for the special [CLS] token  \n",
    " - CLS token: Append a special <CLS> token to the start of every sequence. This special token is meant to capture the sequence-level information. \n",
    " - During the training process, some sentence-level classification (like next sewntence prediction) task based on this CLS embedding will tune the CLS token representation via backpropagation.  \n",
    "  \n",
    "From [article of pooling methods](https://blog.ml6.eu/the-art-of-pooling-embeddings-c56575114cf8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text_list):\n",
    "    encoded_input = tokenizer(\n",
    "        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    # Put to device\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    return cls_pooling(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test one case\n",
    "text_input = comments_dataset['text'][0]\n",
    "embedding = get_embeddings(text_input)\n",
    "# Detach from the computational graph, copy it to host memory, and then convert to numpy array\n",
    "embedding = embedding.detach().cpu().numpy()\n",
    "\n",
    "print(text_input, '\\n', embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute everything; if to select a few:  comments_dataset.select(range(10))\n",
    "embeddings_dataset = comments_dataset.map(\n",
    "    lambda x: {'embeddings': get_embeddings(x['text']).detach().cpu().numpy()[0]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAISS similarity search\n",
    "Using [FAISS](https://faiss.ai/) for efficient similarity search  \n",
    "Here it uses method from hugging face which doesn't have good document or support. # TODO: change to generic FAISS function calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f548dc8c6c38469bb94f560fdce3e6bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length', 'text', 'embeddings'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install faiss-cpu\n",
    "embeddings_dataset.add_faiss_index(column=\"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How can I load a dataset offline?\"\n",
    "question_embedding = get_embeddings([question]).cpu().detach().numpy()\n",
    "question_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, samples = embeddings_dataset.get_nearest_examples(\n",
    "    \"embeddings\", question_embedding, k=5\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "samples_df = pd.DataFrame.from_dict(samples)\n",
    "samples_df[\"scores\"] = scores\n",
    "samples_df.sort_values(\"scores\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMMENT: I just merged a fix, let me know if you're still having this kind of issues :)\n",
      "\n",
      "We'll do a release soon to make this fix available\n",
      "SCORE: 460.86029052734375\n",
      "TITLE: Backwards compatibility broken for cached datasets that use `.filter()`\n",
      "URL: https://github.com/huggingface/datasets/issues/2943\n",
      "==================================================\n",
      "\n",
      "COMMENT: Definitely works on several manual cases with our dummy datasets, thank you @lhoestq !\n",
      "SCORE: 459.71575927734375\n",
      "TITLE: Backwards compatibility broken for cached datasets that use `.filter()`\n",
      "URL: https://github.com/huggingface/datasets/issues/2943\n",
      "==================================================\n",
      "\n",
      "COMMENT: Fixed by #2947.\n",
      "SCORE: 457.87445068359375\n",
      "TITLE: Backwards compatibility broken for cached datasets that use `.filter()`\n",
      "URL: https://github.com/huggingface/datasets/issues/2943\n",
      "==================================================\n",
      "\n",
      "COMMENT: I tried `unshuffled_original_da` and it is also not working\n",
      "SCORE: 415.90667724609375\n",
      "TITLE: OSCAR unshuffled_original_ko: NonMatchingSplitsSizesError\n",
      "URL: https://github.com/huggingface/datasets/issues/2941\n",
      "==================================================\n",
      "\n",
      "COMMENT: Hi @daqieq, thanks for reporting.\n",
      "\n",
      "Unfortunately, I was not able to reproduce this bug:\n",
      "```ipython\n",
      "In [1]: from datasets import load_dataset\n",
      "   ...: ds = load_dataset('wiki_bio')\n",
      "Downloading: 7.58kB [00:00, 26.3kB/s]\n",
      "Downloading: 2.71kB [00:00, ?B/s]\n",
      "Using custom data configuration default\n",
      "Downloading and preparing dataset wiki_bio/default (download: 318.53 MiB, generated: 736.94 MiB, post-processed: Unknown size, total: 1.03 GiB) to C:\\Users\\username\\.cache\\huggingface\\datasets\\wiki_bio\\default\\\n",
      "1.1.0\\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf87e911c9...\n",
      "Downloading: 334MB [01:17, 4.32MB/s]\n",
      "Dataset wiki_bio downloaded and prepared to C:\\Users\\username\\.cache\\huggingface\\datasets\\wiki_bio\\default\\1.1.0\\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf87e911c9. Subsequent calls will reuse thi\n",
      "s data.\n",
      "```\n",
      "\n",
      "This kind of error messages usually happen because:\n",
      "- Your running Python script hasn't write access to that directory\n",
      "- You have another program (the File Explorer?) already browsing inside that directory\n",
      "SCORE: 344.43035888671875\n",
      "TITLE: load_dataset using default cache on Windows causes PermissionError: [WinError 5] Access is denied\n",
      "URL: https://github.com/huggingface/datasets/issues/2937\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _, row in samples_df.iterrows():\n",
    "    print(f\"COMMENT: {row.comments}\")\n",
    "    print(f\"SCORE: {row.scores}\")\n",
    "    print(f\"TITLE: {row.title}\")\n",
    "    print(f\"URL: {row.html_url}\")\n",
    "    print(\"=\" * 50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 2\n",
    "Refer to https://huggingface.co/docs/transformers/tasks/language_modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8653533af8f044d9a6160d4f968cbaca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75f0a90bd7a0470c8fc9219b26eeb341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Not enough memory on my Legion Y9000P. Need to set pagefile to system managed\n",
    "from transformers import pipeline\n",
    "checkpoint = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "generator = pipeline(\"text-generation\", model=checkpoint, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3min 4s\n",
      "Wall time: 8min 26s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': \"How much memory is needed to run Llama2 \\n\\nAnswer: Llama2 is a relatively lightweight library, and it doesn't require a lot of memory to run. In fact, Llama2 is designed Limited spin faut pap Fürulen mű efectспе Welcome以 politique domin\"}]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ['How much memory is needed to run Llama2 ']\n",
    "%time response = generator(prompt, max_new_tokens=50, num_beams=2, do_sample=True, top_k=5, top_p=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'checkpoint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[1;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[43mcheckpoint\u001b[49m)\n\u001b[0;32m      4\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n\u001b[0;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(checkpoint)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'checkpoint' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(raw_inputs, \n",
    "                   # padding='longest', truncation=True, max_length=128, \n",
    "                   return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(inputs, max_new_tokens=20, num_beams=2, do_sample=True, top_k=5, top_p=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Hello. Who are you?\\n\\nComment: Hello! I'm just an AI designed to assist and communicate with users\"]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "checkpoint = 'codellama/CodeLlama-7b-Python-hf'\n",
    "# generator = pipeline(\"text-generation\", model=checkpoint, device_map='auto')\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint,\n",
    "    # load_in_8bit=True,\n",
    "    # torch_dtype=torch.float16,\n",
    "    offload_folder=\"./save_folder\",  # Need to create this folder anyway\n",
    "    device_map=\"auto\",\n",
    "    # device_map={\"\": 0},  # not enough GPU memory\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\")\n",
    "\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.add_eos_token = True\n",
    "# tokenizer.pad_token_id = 0\n",
    "# tokenizer.padding_side = \"left\"\n",
    "\n",
    "prompt = 'Write a piece of Python code to order a list of numbers'\n",
    "inputs = tokenizer(prompt, \n",
    "                   # padding='longest', truncation=True, max_length=128, \n",
    "                   return_tensors=\"pt\"\n",
    "                  )\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**inputs, max_new_tokens=100)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ['Write a piece of Python code to order a list of numbers']\n",
    "%time generator(prompt, max_new_tokens=50, num_beams=2, do_sample=True, top_k=5, top_p=0.95)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
