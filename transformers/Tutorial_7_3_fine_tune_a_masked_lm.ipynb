{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tune with IMDB data. Course on https://huggingface.co/learn/nlp-course/en/chapter7/3?fw=tf  \n",
    "The fine-tuned model will resemble the given corpus in NLP tasks\n",
    " - Domain adaptation/Transfer learning: This process of fine-tuning a pretrained language model on in-domain data  \n",
    " - Masked language modeling: predict missing word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.98553\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_ckpt)\n",
    "distilbert_num_parameters = model.num_parameters() / 1_000_000\n",
    "print(distilbert_num_parameters)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How masked language model works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> This is a great deal.'\n",
      "'>>> This is a great success.'\n",
      "'>>> This is a great adventure.'\n",
      "'>>> This is a great idea.'\n",
      "'>>> This is a great feat.'\n"
     ]
    }
   ],
   "source": [
    "# Make prediction\n",
    "import torch\n",
    "\n",
    "text = \"This is a great [MASK].\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "token_logits = outputs.logits  # [batch, input_dim, vocab_dim]\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]  # inputs[\"input_ids\"] is two-dim\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "top_5_tokens = torch.topk(mask_token_logits, k=5, dim=1).indices[0].tolist()\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode(token))}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n",
      "{'text': 'There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier\\'s plot are far more complicated... Fortier looks more like Prime Suspect, if we have to spot similarities... The main character is weak and weirdo, but have \"clairvoyance\". People like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it\\'s the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all...', 'label': 1}\n",
      "{'text': 'This movie is a great. The plot is very true to the book which is a classic written by Mark Twain. The movie starts of with a scene where Hank sings a song with a bunch of kids called \"when you stub your toe on the moon\" It reminds me of Sinatra\\'s song High Hopes, it is fun and inspirational. The Music is great throughout and my favorite song is sung by the King, Hank (bing Crosby) and Sir \"Saggy\" Sagamore. OVerall a great family movie or even a great Date movie. This is a movie you can watch over and over again. The princess played by Rhonda Fleming is gorgeous. I love this movie!! If you liked Danny Kaye in the Court Jester then you will definitely like this movie.', 'label': 1}\n",
      "{'text': 'George P. Cosmatos\\' \"Rambo: First Blood Part II\" is pure wish-fulfillment. The United States clearly didn\\'t win the war in Vietnam. They caused damage to this country beyond the imaginable and this movie continues the fairy story of the oh-so innocent soldiers. The only bad guys were the leaders of the nation, who made this war happen. The character of Rambo is perfect to notice this. He is extremely patriotic, bemoans that US-Americans didn\\'t appreciate and celebrate the achievements of the single soldier, but has nothing but distrust for leading officers and politicians. Like every film that defends the war (e.g. \"We Were Soldiers\") also this one avoids the need to give a comprehensible reason for the engagement in South Asia. And for that matter also the reason for every single US-American soldier that was there. Instead, Rambo gets to take revenge for the wounds of a whole nation. It would have been better to work on how to deal with the memories, rather than suppressing them. \"Do we get to win this time?\" Yes, you do.', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "print(imdb_dataset)\n",
    "sample = imdb_dataset['train'].shuffle(seed=42).select(range(3))\n",
    "for i in sample:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate all the examples and then split the whole corpus into chunks of equal size. \n",
    "For corpus with sentences of various length, padding and truncating each sentence directly will waste space and lose info. Concat and then split can avoid that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "406b818a5e7843db8b43d2f7ba0ee6dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# First tokenize the corpus as usual\n",
    "def tokenize_function(examples):\n",
    "    '''tokenize text, and save the word id of the whole word for word masking later '''\n",
    "    result = tokenizer(examples[\"text\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "tokenized_datasets = imdb_dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True,\n",
    "    remove_columns=[\"text\", \"label\"]\n",
    ")\n",
    "\n",
    "print(tokenized_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum context size is: 512; Choose a smaller chunk size for GPU limitation: 128\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ca7b44a378435f8152227d01bc31b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 59904\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 61291\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 122957\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then concatenate and split\n",
    "chunk_size = 128\n",
    "print(f\"The maximum context size is: {tokenizer.model_max_length}; Choose a smaller chunk size for GPU limitation: {chunk_size}\")\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}  # why using .keys()?\n",
    "\n",
    "    # Drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = len(concatenated_examples[list(concatenated_examples.keys())[0]])\n",
    "    total_length = total_length // chunk_size * chunk_size\n",
    "\n",
    "    # Split by chunks\n",
    "    result = {\n",
    "        k: [v[i:i + chunk_size] for i in range(0, total_length, chunk_size)] for k, v in concatenated_examples.items()\n",
    "    }\n",
    "\n",
    "    # Create a new labels column as the input_ids. This is the ground truth in preidcting masked tokens\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    \n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ) of swedish cinema. but really, this film doesn't have much of a plot. [SEP] [CLS] \" i am curious : yellow \" is a risible and pretentious steaming pile. it doesn\n"
     ]
    }
   ],
   "source": [
    "# You can see the sentence breaker [SEP] and [CLS]\n",
    "print(tokenizer.decode(lm_datasets[\"train\"][2][\"input_ids\"])[400:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask tokens as the training target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> [CLS] i rented i hooker curious - yellow from my video [MASK] because [MASK] all the controversy that surrounded it when it was first [MASK] in [MASK]. i also heard that at [MASK] it was seized by u. s. customs if it ever tried to enter this country, [MASK] being a fan of films considered \" controversial [MASK] i really had to see this for myself. [MASK] br / > < br / > the plot is centered around a young swedish drama [MASK] [MASK] lena who wants to learn everything she can about life. in particular she wants to focus [MASK] attentions to making some sort of documentary on what the average [MASK]ede thought about certain political issues such\n",
      "\n",
      ">>> ['[CLS]', 'i', 'rented', 'i', 'hooker', 'curious', '-', 'yellow', 'from', 'my', 'video', '[MASK]', 'because', '[MASK]', 'all', 'the', 'controversy', 'that', 'surrounded', 'it', 'when', 'it', 'was', 'first', '[MASK]', 'in', '[MASK]', '.', 'i', 'also', 'heard', 'that', 'at', '[MASK]', 'it', 'was', 'seized', 'by', 'u', '.', 's', '.', 'customs', 'if', 'it', 'ever', 'tried', 'to', 'enter', 'this', 'country', ',', '[MASK]', 'being', 'a', 'fan', 'of', 'films', 'considered', '\"', 'controversial', '[MASK]', 'i', 'really', 'had', 'to', 'see', 'this', 'for', 'myself', '.', '[MASK]', 'br', '/', '>', '<', 'br', '/', '>', 'the', 'plot', 'is', 'centered', 'around', 'a', 'young', 'swedish', 'drama', '[MASK]', '[MASK]', 'lena', 'who', 'wants', 'to', 'learn', 'everything', 'she', 'can', 'about', 'life', '.', 'in', 'particular', 'she', 'wants', 'to', 'focus', '[MASK]', 'attention', '##s', 'to', 'making', 'some', 'sort', 'of', 'documentary', 'on', 'what', 'the', 'average', '[MASK]', '##ede', 'thought', 'about', 'certain', 'political', 'issues', 'such']\n",
      "\n",
      ">>> boogie the vietnam war and race issues [MASK] the united states. in [MASK] asking politicians and ordinary denizens of stockholm [MASK] their opinions [MASK] politics, she has sex with her drama [MASK], classmates, and married men. < br / > < br / > what [MASK] me about [MASK] am curious - yellow [MASK] [MASK] 40 years ago [MASK] this was considered pornographic. really, the sex [MASK] [MASK] [MASK] [MASK] are few [MASK] far between, even theninates's [MASK] shot [MASK] some cheaply made porno. while my countrymen mind find itkaya [MASK] in [MASK] sex and nudity [MASK] a major staple in swedish [MASK] [MASK] even ingmar bergman,\n",
      "\n",
      ">>> ['boogie', 'the', 'vietnam', 'war', 'and', 'race', 'issues', '[MASK]', 'the', 'united', 'states', '.', 'in', '[MASK]', 'asking', 'politicians', 'and', 'ordinary', 'den', '##ize', '##ns', 'of', 'stockholm', '[MASK]', 'their', 'opinions', '[MASK]', 'politics', ',', 'she', 'has', 'sex', 'with', 'her', 'drama', '[MASK]', ',', 'classmates', ',', 'and', 'married', 'men', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'what', '[MASK]', 'me', 'about', '[MASK]', 'am', 'curious', '-', 'yellow', '[MASK]', '[MASK]', '40', 'years', 'ago', '[MASK]', 'this', 'was', 'considered', 'pornographic', '.', 'really', ',', 'the', 'sex', '[MASK]', '[MASK]', '[MASK]', '[MASK]', 'are', 'few', '[MASK]', 'far', 'between', ',', 'even', 'then', '##inates', \"'\", 's', '[MASK]', 'shot', '[MASK]', 'some', 'cheap', '##ly', 'made', 'porn', '##o', '.', 'while', 'my', 'country', '##men', 'mind', 'find', 'it', '##kaya', '[MASK]', 'in', '[MASK]', 'sex', 'and', 'nu', '##dity', '[MASK]', 'a', 'major', 'staple', 'in', 'swedish', '[MASK]', '[MASK]', 'even', 'ing', '##mar', 'bergman', ',']\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Token masking\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)  # Mask 15% of the tokens randomly\n",
    "\n",
    "# Do a test\n",
    "samples = [lm_datasets['train'][i] for i in range(2)]  # Need a list here so can't just slice lm_datasets\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n>>> {tokenizer.decode(chunk)}\")\n",
    "    print(f\"\\n>>> {tokenizer.convert_ids_to_tokens(chunk)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Whole world masking\n",
    "import collections\n",
    "import numpy as np\n",
    "from transformers import default_data_collator\n",
    "\n",
    "wwm_probability = 0.2\n",
    "\n",
    "\n",
    "def whole_word_masking_data_collator(features):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\")\n",
    "\n",
    "        # Create a map between words and corresponding token indices\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):  # idx correponds to the index of tokens\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)  # mapping the index of word to that of tokens\n",
    "\n",
    "        # Randomly mask words\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))  # Binomial distribution\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:  # loop over the word index where mask is 1\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:  # get the correponding token indices from mapping\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "        feature[\"labels\"] = new_labels\n",
    "\n",
    "    return default_data_collator(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downsampling data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downsampling and train test split\n",
    "is_whole_word_masking = False  # The last section (Train with Acccelerate) is for token masking only\n",
    "train_size = 50_000\n",
    "test_size = int(0.1 * train_size)\n",
    "\n",
    "if is_whole_word_masking:\n",
    "    run_datasets = lm_datasets[\"train\"]\n",
    "    run_data_collator = whole_word_masking_data_collator\n",
    "else:\n",
    "    run_datasets = lm_datasets[\"train\"].remove_columns(\"word_ids\")\n",
    "    run_data_collator = data_collator\n",
    "\n",
    "downsampled_dataset = run_datasets.train_test_split(\n",
    "    train_size=train_size, test_size=test_size, seed=42\n",
    ")\n",
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Login HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b687cb9935e64e22b97b92ba316262dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_ckpt.split(\"/\")[-1]\n",
    "output_dir = f\"{model_name}-finetuned-imdb\"\n",
    "hub_model_id = f\"hf-nlp-course-{model_name}-finetuned-imdb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Trainer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "batch_size = 64\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=3,  # default is 3\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    fp16=True,  # enable mixed-precision training, which gives a boost in speed\n",
    "    logging_steps=logging_steps,\n",
    "    remove_unused_columns=False,  # By default, the Trainer will remove any columns that are not part of the model’s forward() method. \n",
    "    # So if you’re using the whole word masking collator, you’ll also need to set remove_unused_columns=False to ensure we don’t lose the word_ids column during training.\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=hub_model_id,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    data_collator=run_data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7541804ad244ff81f3645311730fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of the original model: 10.84\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8188d954824e48198024cf11536507a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2346 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.496, 'learning_rate': 1.3341858482523444e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de802460951249b79b45e4d7357c963d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.311464309692383, 'eval_runtime': 10.1976, 'eval_samples_per_second': 490.314, 'eval_steps_per_second': 7.747, 'epoch': 1.0}\n",
      "{'loss': 2.4268, 'learning_rate': 6.683716965046889e-06, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b034722492c244c6af937cf48ea95f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2842066287994385, 'eval_runtime': 10.3646, 'eval_samples_per_second': 482.409, 'eval_steps_per_second': 7.622, 'epoch': 2.0}\n",
      "{'loss': 2.3915, 'learning_rate': 2.5575447570332485e-08, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89dd8703cf1348e09cfd9919e6b90386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2513575553894043, 'eval_runtime': 8.6924, 'eval_samples_per_second': 575.213, 'eval_steps_per_second': 9.088, 'epoch': 3.0}\n",
      "{'train_runtime': 1717.3667, 'train_samples_per_second': 87.343, 'train_steps_per_second': 1.366, 'train_loss': 2.438064855891014, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74eec2b9633b45ddbb4e722a977338e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity after training: 9.75\n"
     ]
    }
   ],
   "source": [
    "# Evaluation using perplexity\n",
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "ppl = math.exp(eval_results['eval_loss'])\n",
    "print(f\"Perplexity of the original model: {ppl:.2f}\")\n",
    "\n",
    "trainer.train()\n",
    "eval_results = trainer.evaluate()\n",
    "ppl = math.exp(eval_results['eval_loss'])\n",
    "print(f\"Perplexity after training: {ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f74f074ec6fa4c7195dbd8781e1f1d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/yuwei2342/hf-nlp-course-distilbert-base-uncased-finetuned-imdb/tree/main/'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Accelerate for customization\n",
    "\n",
    "### Create an eval dataset with random masking\n",
    "In the above run, DataCollatorForLanguageModeling applies random masking with eval_dataset for every epoch, which adds randomness in evaulation.  \n",
    "The follows apply the masking once on the whole test set, and then use the default data collator in HF Transformers to collect the batches during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2f6bb1f2514e11a3e51a66d25c15d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def insert_random_mask(batch):\n",
    "    # dict of lists -> list: data_collator can only take list\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    # use collator to insert random mask\n",
    "    masked_inputs = data_collator(features)\n",
    "    # Create a new \"masked\" column for each column in the dataset\n",
    "    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}\n",
    "\n",
    "eval_dataset = downsampled_dataset[\"test\"].map(\n",
    "    insert_random_mask,\n",
    "    batched=True,\n",
    "    remove_columns=downsampled_dataset[\"test\"].column_names,\n",
    ")\n",
    "eval_dataset = eval_dataset.rename_columns(\n",
    "    {\n",
    "        \"masked_input_ids\": \"input_ids\",\n",
    "        \"masked_attention_mask\": \"attention_mask\",\n",
    "        \"masked_labels\": \"labels\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for training: Create data loader, config model, define eval metrics,nd setup hf hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loader. Use default data collator for eval data since it's already masked\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(\n",
    "    downsampled_dataset[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, \n",
    "    batch_size=batch_size, \n",
    "    collate_fn=default_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config model, optimizer and schedular\n",
    "from torch.optim import AdamW \n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_ckpt)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "\n",
    "# Prepare for training with Accelerator\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# Accelerator sets up distributed training, so as to replace the follows\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); model.to(device); inputs = inputs.to(device)\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader,\n",
    ")\n",
    "\n",
    "\n",
    "# Set lr schedule\n",
    "from transformers import get_scheduler\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",  \n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics\n",
    "def compute_loss_ppl(model, eval_dataloader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        # accelerator.gather collects predictions from all distributed processors and cat them\n",
    "        # loss usually is a scalar (0-dim), this causes problem when gathering. Here .repeat creates a new tensor by repeating the original loss tensor batch_size times.\n",
    "        losses.append(accelerator.gather(loss.repeat(batch_size)))  \n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    # This code trims the gathered losses to match the size of the evaluation dataset, in case extra losses beyond the actual number of evaluation samples.\n",
    "    losses = losses[: len(eval_dataloader.dataset)]\n",
    "    try:\n",
    "        perplexity = math.exp(torch.mean(losses))\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "\n",
    "    return losses, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Code\\nlp\\transformers\\manual-distilbert-base-uncased-finetuned-imdb is already a clone of https://huggingface.co/yuwei2342/hf-nlp-course-distilbert-base-uncased-finetuned-imdb. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "# Setup hugging face hub\n",
    "\n",
    "# import huggingface_hub as hfh\n",
    "# hfh.delete_repo(repo_id='yuwei2342/distilbert-base-uncased-finetuned-imdb')\n",
    "\n",
    "# Close the repo to push to to local\n",
    "from huggingface_hub import get_full_repo_name, Repository\n",
    "repo_name = get_full_repo_name(hub_model_id)\n",
    "output_dir = f\"manual-{output_dir}\"\n",
    "repo = Repository(output_dir, clone_from=repo_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training: Perplexity: 23.057392216984677\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ec9461db5c47c0bc235b2411d313f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/237 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Perplexity: 12.073483717062386\n",
      "Epoch 1: Perplexity: 11.538012292958447\n",
      "Epoch 2: Perplexity: 11.399004637899463\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import math\n",
    "\n",
    "_, perplexity = compute_loss_ppl(model, eval_dataloader)\n",
    "print(f\"Before training: Perplexity: {perplexity}\")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Eval\n",
    "    _, perplexity = compute_loss_ppl(model, eval_dataloader)\n",
    "    print(f\"Epoch {epoch}: Perplexity: {perplexity}\")\n",
    "\n",
    "    # Save and upload, equivalent to trainer.push_to_hub()\n",
    "    accelerator.wait_for_everyone()\n",
    "    # This reverses accelerator.prepare(model)\n",
    "    unwrapped_model = accelerator.unwrap_model(model)  \n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        repo.push_to_hub(\n",
    "            commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run fine-tuned model with pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f5114464f64e4198c744b3e9316bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/581 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Code\\env\\penv\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\Code\\.cache\\huggingface\\transformers. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be9d960b68a46a2808e96a878e7bd35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66ea1c52e4d940c5acf15a1eac63ceff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/333 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de966e34596e466d9d3ccd2e7d26e434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b186275af07940be81f61c000d79dfb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d67936846964c8ba1807e9e3aba6fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> this is a great film.\n",
      ">>> this is a great movie.\n",
      ">>> this is a great idea.\n",
      ">>> this is a great show.\n",
      ">>> this is a great story.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "mask_filler = pipeline(\n",
    "    \"fill-mask\", \n",
    "    model=repo_name,  # output_dir\n",
    ")\n",
    "\n",
    "preds = mask_filler(text)\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
