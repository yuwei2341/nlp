{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ae5d48f-8444-4aa8-8138-25487c80cd0d",
   "metadata": {},
   "source": [
    "# Train a new tokenizer from an old one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd22b04-1479-4205-80e1-c64321b9a202",
   "metadata": {},
   "source": [
    "## Load the code corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56911753-bdf6-40ae-8e75-ca1bf6b5c264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "    num_rows: 412178\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset('code_search_net', 'python')\n",
    "raw_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dae22760-8934-460f-8783-6ce783cf58e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repository_name ageitgey/face_recognition\n",
      "func_path_in_repository face_recognition/api.py\n",
      "func_name _trim_css_to_bounds\n",
      "whole_func_string def _trim_css_to_bounds(css, image_shape):\n",
      "    \"\"\"\n",
      "    Make sure a tuple in (top, right, bottom, left) order is within the bounds of the image.\n",
      "\n",
      "    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order\n",
      "    :param image_shape: numpy shape of the image array\n",
      "    :return: a trimmed plain tuple representation of the rect in (top, right, bottom, left) order\n",
      "    \"\"\"\n",
      "    return max(css[0], 0), min(css[1], image_shape[1]), min(css[2], image_shape[0]), max(css[3], 0)\n",
      "language python\n",
      "func_code_string def _trim_css_to_bounds(css, image_shape):\n",
      "    \"\"\"\n",
      "    Make sure a tuple in (top, right, bottom, left) order is within the bounds of the image.\n",
      "\n",
      "    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order\n",
      "    :param image_shape: numpy shape of the image array\n",
      "    :return: a trimmed plain tuple representation of the rect in (top, right, bottom, left) order\n",
      "    \"\"\"\n",
      "    return max(css[0], 0), min(css[1], image_shape[1]), min(css[2], image_shape[0]), max(css[3], 0)\n",
      "func_code_tokens ['def', '_trim_css_to_bounds', '(', 'css', ',', 'image_shape', ')', ':', 'return', 'max', '(', 'css', '[', '0', ']', ',', '0', ')', ',', 'min', '(', 'css', '[', '1', ']', ',', 'image_shape', '[', '1', ']', ')', ',', 'min', '(', 'css', '[', '2', ']', ',', 'image_shape', '[', '0', ']', ')', ',', 'max', '(', 'css', '[', '3', ']', ',', '0', ')']\n",
      "func_documentation_string Make sure a tuple in (top, right, bottom, left) order is within the bounds of the image.\n",
      "\n",
      "    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order\n",
      "    :param image_shape: numpy shape of the image array\n",
      "    :return: a trimmed plain tuple representation of the rect in (top, right, bottom, left) order\n",
      "func_documentation_tokens ['Make', 'sure', 'a', 'tuple', 'in', '(', 'top', 'right', 'bottom', 'left', ')', 'order', 'is', 'within', 'the', 'bounds', 'of', 'the', 'image', '.']\n",
      "split_name train\n",
      "func_code_url https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L52-L60\n"
     ]
    }
   ],
   "source": [
    "for k, v in raw_datasets[\"train\"][4].items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1fa9ef-7838-4c84-bb80-d58b49f53d75",
   "metadata": {},
   "source": [
    "## Create a Python generator to load only batches to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39b37264-6133-46b9-8379-2aa4d6d60786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generator"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These two ways of defining generators are equivalent\n",
    "def get_training_corpus():\n",
    "    dataset = raw_datasets[\"train\"]\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx : start_idx + 1000]\n",
    "        yield samples[\"whole_func_string\"]\n",
    "        \n",
    "def get_training_corpus():\n",
    "    return (\n",
    "        raw_datasets['train'][i : i + 1000]['whole_func_string']\n",
    "        for i in range(0, len(raw_datasets['train']), 1000)\n",
    "    )\n",
    "\n",
    "training_corpus = get_training_corpus()\n",
    "type(training_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af5b700-83a4-4fa9-9291-581ef9453c41",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd43d7fe-c068-4363-9697-5854639f4acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġadd', '_', 'n', 'umbers', '(', 'a', ',', 'Ġb', '):', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ\"\"\"', 'Add', 'Ġthe', 'Ġtwo', 'Ġnumbers', 'Ġ`', 'a', '`', 'Ġand', 'Ġ`', 'b', '`', '.\"', '\"\"', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġreturn', 'Ġa', 'Ġ+', 'Ġb']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "old_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "example = '''def add_numbers(a, b):\n",
    "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
    "    return a + b'''\n",
    "\n",
    "tokens = old_tokenizer.tokenize(example)\n",
    "# Ġ is spaces, Ċ is newlines\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf80ced6-0e2b-4ff0-91cb-badfae24bfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bea5596e-1669-4f97-b7ee-168087f0e666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġadd', '_', 'numbers', '(', 'a', ',', 'Ġb', '):', 'ĊĠĠĠ', 'Ġ\"\"\"', 'Add', 'Ġthe', 'Ġtwo', 'Ġnumbers', 'Ġ`', 'a', '`', 'Ġand', 'Ġ`', 'b', '`.\"\"\"', 'ĊĠĠĠ', 'Ġreturn', 'Ġa', 'Ġ+', 'Ġb']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f316e2-a0c7-40c7-8381-c415c78cac20",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2825ff2-7142-45c0-86ea-56b3ce44d412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('.model/code-search-net-tokenizer\\\\tokenizer_config.json',\n",
       " '.model/code-search-net-tokenizer\\\\special_tokens_map.json',\n",
       " '.model/code-search-net-tokenizer\\\\vocab.json',\n",
       " '.model/code-search-net-tokenizer\\\\merges.txt',\n",
       " '.model/code-search-net-tokenizer\\\\added_tokens.json',\n",
       " '.model/code-search-net-tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('.model/code-search-net-tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2df0c20-eb18-4241-bbd1-84d01ebc23f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "# tokenizer.push_to_hub(\"code-search-net-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00c5071-862b-45b6-b7b6-0dd6b21c918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Next: https://huggingface.co/learn/nlp-course/chapter6/3?fw=pt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
