{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ae5d48f-8444-4aa8-8138-25487c80cd0d",
   "metadata": {},
   "source": [
    "# Train a new tokenizer from an old one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd22b04-1479-4205-80e1-c64321b9a202",
   "metadata": {},
   "source": [
    "## Load the code corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56911753-bdf6-40ae-8e75-ca1bf6b5c264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "    num_rows: 412178\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset('code_search_net', 'python')\n",
    "raw_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dae22760-8934-460f-8783-6ce783cf58e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repository_name ageitgey/face_recognition\n",
      "func_path_in_repository face_recognition/api.py\n",
      "func_name _trim_css_to_bounds\n",
      "whole_func_string def _trim_css_to_bounds(css, image_shape):\n",
      "    \"\"\"\n",
      "    Make sure a tuple in (top, right, bottom, left) order is within the bounds of the image.\n",
      "\n",
      "    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order\n",
      "    :param image_shape: numpy shape of the image array\n",
      "    :return: a trimmed plain tuple representation of the rect in (top, right, bottom, left) order\n",
      "    \"\"\"\n",
      "    return max(css[0], 0), min(css[1], image_shape[1]), min(css[2], image_shape[0]), max(css[3], 0)\n",
      "language python\n",
      "func_code_string def _trim_css_to_bounds(css, image_shape):\n",
      "    \"\"\"\n",
      "    Make sure a tuple in (top, right, bottom, left) order is within the bounds of the image.\n",
      "\n",
      "    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order\n",
      "    :param image_shape: numpy shape of the image array\n",
      "    :return: a trimmed plain tuple representation of the rect in (top, right, bottom, left) order\n",
      "    \"\"\"\n",
      "    return max(css[0], 0), min(css[1], image_shape[1]), min(css[2], image_shape[0]), max(css[3], 0)\n",
      "func_code_tokens ['def', '_trim_css_to_bounds', '(', 'css', ',', 'image_shape', ')', ':', 'return', 'max', '(', 'css', '[', '0', ']', ',', '0', ')', ',', 'min', '(', 'css', '[', '1', ']', ',', 'image_shape', '[', '1', ']', ')', ',', 'min', '(', 'css', '[', '2', ']', ',', 'image_shape', '[', '0', ']', ')', ',', 'max', '(', 'css', '[', '3', ']', ',', '0', ')']\n",
      "func_documentation_string Make sure a tuple in (top, right, bottom, left) order is within the bounds of the image.\n",
      "\n",
      "    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order\n",
      "    :param image_shape: numpy shape of the image array\n",
      "    :return: a trimmed plain tuple representation of the rect in (top, right, bottom, left) order\n",
      "func_documentation_tokens ['Make', 'sure', 'a', 'tuple', 'in', '(', 'top', 'right', 'bottom', 'left', ')', 'order', 'is', 'within', 'the', 'bounds', 'of', 'the', 'image', '.']\n",
      "split_name train\n",
      "func_code_url https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L52-L60\n"
     ]
    }
   ],
   "source": [
    "for k, v in raw_datasets[\"train\"][4].items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1fa9ef-7838-4c84-bb80-d58b49f53d75",
   "metadata": {},
   "source": [
    "## Create a Python generator to load only batches to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39b37264-6133-46b9-8379-2aa4d6d60786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generator"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These two ways of defining generators are equivalent\n",
    "def get_training_corpus():\n",
    "    dataset = raw_datasets[\"train\"]\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx : start_idx + 1000]\n",
    "        yield samples[\"whole_func_string\"]\n",
    "        \n",
    "def get_training_corpus():\n",
    "    return (\n",
    "        raw_datasets['train'][i : i + 1000]['whole_func_string']\n",
    "        for i in range(0, len(raw_datasets['train']), 1000)\n",
    "    )\n",
    "\n",
    "training_corpus = get_training_corpus()\n",
    "type(training_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af5b700-83a4-4fa9-9291-581ef9453c41",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd43d7fe-c068-4363-9697-5854639f4acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'ƒ†add', '_', 'n', 'umbers', '(', 'a', ',', 'ƒ†b', '):', 'ƒä', 'ƒ†', 'ƒ†', 'ƒ†', 'ƒ†\"\"\"', 'Add', 'ƒ†the', 'ƒ†two', 'ƒ†numbers', 'ƒ†`', 'a', '`', 'ƒ†and', 'ƒ†`', 'b', '`', '.\"', '\"\"', 'ƒä', 'ƒ†', 'ƒ†', 'ƒ†', 'ƒ†return', 'ƒ†a', 'ƒ†+', 'ƒ†b']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "old_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "example = '''def add_numbers(a, b):\n",
    "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
    "    return a + b'''\n",
    "\n",
    "tokens = old_tokenizer.tokenize(example)\n",
    "# ƒ† is spaces, ƒä is newlines\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf80ced6-0e2b-4ff0-91cb-badfae24bfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bea5596e-1669-4f97-b7ee-168087f0e666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'ƒ†add', '_', 'numbers', '(', 'a', ',', 'ƒ†b', '):', 'ƒäƒ†ƒ†ƒ†', 'ƒ†\"\"\"', 'Add', 'ƒ†the', 'ƒ†two', 'ƒ†numbers', 'ƒ†`', 'a', '`', 'ƒ†and', 'ƒ†`', 'b', '`.\"\"\"', 'ƒäƒ†ƒ†ƒ†', 'ƒ†return', 'ƒ†a', 'ƒ†+', 'ƒ†b']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f316e2-a0c7-40c7-8381-c415c78cac20",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2825ff2-7142-45c0-86ea-56b3ce44d412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('.model/code-search-net-tokenizer\\\\tokenizer_config.json',\n",
       " '.model/code-search-net-tokenizer\\\\special_tokens_map.json',\n",
       " '.model/code-search-net-tokenizer\\\\vocab.json',\n",
       " '.model/code-search-net-tokenizer\\\\merges.txt',\n",
       " '.model/code-search-net-tokenizer\\\\added_tokens.json',\n",
       " '.model/code-search-net-tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('.model/code-search-net-tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2df0c20-eb18-4241-bbd1-84d01ebc23f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "# tokenizer.push_to_hub(\"code-search-net-tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17164c4-a2ac-433d-b303-798c598f5c63",
   "metadata": {},
   "source": [
    "# Fast tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fd3c16-52e5-4d23-92dc-17c54a50d078",
   "metadata": {},
   "source": [
    "## Fast tokenizers: batch encoding, offset mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dce35530-6641-4e11-9235-9125a338f669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "['_MutableMapping__marker', '__abstractmethods__', '__class__', '__class_getitem__', '__contains__', '__copy__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__ior__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__or__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__ror__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_encodings', '_n_sequences', 'char_to_token', 'char_to_word', 'clear', 'convert_to_tensors', 'copy', 'data', 'encodings', 'fromkeys', 'get', 'is_fast', 'items', 'keys', 'n_sequences', 'pop', 'popitem', 'sequence_ids', 'setdefault', 'to', 'token_to_chars', 'token_to_sequence', 'token_to_word', 'tokens', 'update', 'values', 'word_ids', 'word_to_chars', 'word_to_tokens', 'words']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "example = 'My name is Sylvain and I work at Hugging Face in Brooklyn.'\n",
    "encoding = tokenizer(example)\n",
    "print(type(encoding))\n",
    "print(dir(encoding))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4a2cc3-5cc9-4224-8eea-10692cd1db1a",
   "metadata": {},
   "source": [
    "### Mapping between token, ids and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0859a4ed-76da-4578-844f-822caee73276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in', 'Brooklyn', '.', '[SEP]']\n",
      "[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(encoding.tokens())\n",
    "print(encoding.word_ids())\n",
    "print(encoding.token_type_ids)  # like sentence id here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8d018842-7733-4e74-8c71-90f27821d823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sylvain\n",
      "##yl yl\n",
      "M My\n",
      "S ['S', '##yl', '##va', '##in']\n"
     ]
    }
   ],
   "source": [
    "start, end = encoding.word_to_chars(3)  # From word id to the original string\n",
    "print(example[start:end])\n",
    "\n",
    "start, end = encoding.token_to_chars(5)  # From tokens to the original string\n",
    "print(encoding.tokens()[5], example[start:end])\n",
    "\n",
    "ind = encoding.char_to_token(0)  # From the original string to tokens\n",
    "print(example[0], encoding.tokens()[ind])\n",
    "\n",
    "ind = encoding.char_to_word(11)  # From the original string to word id\n",
    "word_ind = [i for i, j in enumerate(encoding.word_ids()) if j == ind]\n",
    "print(example[11], [encoding.tokens()[i] for i in word_ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff15a6f-7c9e-465c-9429-feec2b4b963d",
   "metadata": {},
   "source": [
    "## Token-classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325ab032-6850-45c3-a0a6-b6224f988e56",
   "metadata": {},
   "source": [
    "### As a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5b118665-616e-441f-ac13-681bfba6261e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'I-PER', 'score': 0.99938285, 'index': 4, 'word': 'S', 'start': 11, 'end': 12}, {'entity': 'I-PER', 'score': 0.99815494, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14}, {'entity': 'I-PER', 'score': 0.99590707, 'index': 6, 'word': '##va', 'start': 14, 'end': 16}, {'entity': 'I-PER', 'score': 0.99923277, 'index': 7, 'word': '##in', 'start': 16, 'end': 18}, {'entity': 'I-ORG', 'score': 0.9738931, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35}, {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40}, {'entity': 'I-ORG', 'score': 0.9887976, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45}, {'entity': 'I-LOC', 'score': 0.9932106, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentence = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "# by default checkpoint = 'dbmdz/bert-large-cased-finetuned-conll03-english'\n",
    "token_classifier = pipeline('token-classification')\n",
    "print(token_classifier(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ade80f8b-ef49-4421-b363-420f7a9ea600",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9981694,\n",
       "  'word': 'Sylvain',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9796019,\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9932106,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_classifier = pipeline('token-classification', \n",
    "                            aggregation_strategy='simple',  \n",
    "                            # 'first', 'max', 'average'\n",
    ")\n",
    "token_classifier(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097124cf-af15-412a-8f03-cdfeba5d1eaf",
   "metadata": {},
   "source": [
    "### Step by step - From inputs to predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5a8430b8-8486-4435-874e-9cb5036ca7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "checkpoint = 'dbmdz/bert-large-cased-finetuned-conll03-english'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(checkpoint)\n",
    "\n",
    "inputs = tokenizer(sentence, return_tensors='pt')\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "49b1852b-d0cd-429d-aa01-83ec2ab01146",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 19])\n",
      "torch.Size([1, 19, 9])\n"
     ]
    }
   ],
   "source": [
    "print(inputs['input_ids'].shape)\n",
    "print(outputs['logits'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d7adc9ad-bb80-462a-9011-cc960348470e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]\n",
      "{0: 'O', 1: 'B-MISC', 2: 'I-MISC', 3: 'B-PER', 4: 'I-PER', 5: 'B-ORG', 6: 'I-ORG', 7: 'B-LOC', 8: 'I-LOC'}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()\n",
    "predictions = outputs.logits.argmax(dim=-1)[0].tolist()\n",
    "print(predictions)\n",
    "\n",
    "# Label of tokens\n",
    "# O is the label for the tokens that are not in any named entity (it stands for ‚Äúoutside‚Äù)\n",
    "# In the IOB1 format, the labels beginning with B- are only ever used to separate two adjacent entities of the same type. \n",
    "\n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "96d63521-43fd-49f9-be85-a65a1a0cc1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-PER', 'score': 0.9993828535079956, 'word': 'S'},\n",
       " {'entity': 'I-PER', 'score': 0.9981549382209778, 'word': '##yl'},\n",
       " {'entity': 'I-PER', 'score': 0.995907187461853, 'word': '##va'},\n",
       " {'entity': 'I-PER', 'score': 0.9992327690124512, 'word': '##in'},\n",
       " {'entity': 'I-ORG', 'score': 0.9738931059837341, 'word': 'Hu'},\n",
       " {'entity': 'I-ORG', 'score': 0.9761149883270264, 'word': '##gging'},\n",
       " {'entity': 'I-ORG', 'score': 0.9887976050376892, 'word': 'Face'},\n",
       " {'entity': 'I-LOC', 'score': 0.9932106137275696, 'word': 'Brooklyn'}]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "tokens = inputs.tokens()\n",
    "\n",
    "for idx, pred in enumerate(predictions):\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != 'O':\n",
    "        results.append(\n",
    "            {'entity': label, 'score': probabilities[idx][pred], 'word': tokens[idx]}\n",
    "    )\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5b5f8184-2838-408e-83df-a3cedbed00da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 2), (3, 7), (8, 10), (11, 12), (12, 14), (14, 16), (16, 18), (19, 22), (23, 24), (25, 29), (30, 32), (33, 35), (35, 40), (41, 45), (46, 48), (49, 57), (57, 58), (0, 0)]\n",
      "[{'entity': 'I-PER', 'score': 0.9993828535079956, 'word': 'S', 'start': 11, 'end': 12}, {'entity': 'I-PER', 'score': 0.9981549382209778, 'word': '##yl', 'start': 12, 'end': 14}, {'entity': 'I-PER', 'score': 0.995907187461853, 'word': '##va', 'start': 14, 'end': 16}, {'entity': 'I-PER', 'score': 0.9992327690124512, 'word': '##in', 'start': 16, 'end': 18}, {'entity': 'I-ORG', 'score': 0.9738931059837341, 'word': 'Hu', 'start': 33, 'end': 35}, {'entity': 'I-ORG', 'score': 0.9761149883270264, 'word': '##gging', 'start': 35, 'end': 40}, {'entity': 'I-ORG', 'score': 0.9887976050376892, 'word': 'Face', 'start': 41, 'end': 45}, {'entity': 'I-LOC', 'score': 0.9932106137275696, 'word': 'Brooklyn', 'start': 49, 'end': 57}]\n"
     ]
    }
   ],
   "source": [
    "# Get offests mapping to map to original sentence\n",
    "inputs_with_offsets = tokenizer(sentence, return_offsets_mapping=True)\n",
    "offsets = inputs_with_offsets['offset_mapping']\n",
    "print(offsets)\n",
    "\n",
    "results = []\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "\n",
    "for idx, pred in enumerate(predictions):\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != 'O':\n",
    "        start, end = offsets[idx]\n",
    "        results.append(\n",
    "            {'entity': label, \n",
    "             'score': probabilities[idx][pred], \n",
    "             'word': tokens[idx],\n",
    "             'start': start,\n",
    "             'end': end,\n",
    "            }\n",
    "    )\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a37a156-9ffa-4240-8df9-b18d867e2d59",
   "metadata": {},
   "source": [
    "### Group the entities with offset mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f36af180-f27b-4070-9957-a9c3d9ae45f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'PER', 'score': 0.9981694370508194, 'word': 'Sylvain', 'start': 11, 'end': 18}, {'entity': 'ORG', 'score': 0.9796018997828165, 'word': 'Hugging Face', 'start': 33, 'end': 45}, {'entity': 'LOC', 'score': 0.9932106137275696, 'word': 'Brooklyn', 'start': 49, 'end': 57}]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "results = []\n",
    "inputs_with_offsets = tokenizer(sentence, return_offsets_mapping=True)\n",
    "offsets = inputs_with_offsets['offset_mapping']\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "\n",
    "idx = 0\n",
    "while idx < len(predictions):\n",
    "    pred = predictions[idx]\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != 'O':\n",
    "        # Remove the B- or I- prefix\n",
    "        label = label[2:]\n",
    "        start, _ = offsets[idx]\n",
    "\n",
    "        all_scores = []\n",
    "        while (\n",
    "            idx < len(predictions)\n",
    "            and model.config.id2label[predictions[idx]] == f'I-{label}'\n",
    "        ):\n",
    "            all_scores.append(probabilities[idx][pred])\n",
    "            _, end = offsets[idx]\n",
    "            idx += 1\n",
    "\n",
    "        # The score is the mean of all the scores of the tokens in that grouped entity\n",
    "        score = np.mean(all_scores).item()\n",
    "        word = sentence[start:end]\n",
    "        results.append(\n",
    "            {'entity': label, \n",
    "             'score': score, \n",
    "             'word': word,\n",
    "             'start': start,\n",
    "             'end': end,\n",
    "            }\n",
    "        )\n",
    "    idx += 1\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f66b67-7456-4966-895e-df9cf3cb6410",
   "metadata": {},
   "source": [
    "# Fast tokenizers in the QA pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c96377-b5e9-46d1-85ad-dd56216b6d1b",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "015fe2c1-f481-4be1-890a-5c613411c7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.9779755473136902,\n",
       " 'start': 89,\n",
       " 'end': 117,\n",
       " 'answer': 'Jax, PyTorch, and TensorFlow'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline('question-answering')\n",
    "context = \"\"\"\n",
    "Hugging Face Transformers is backed by the three most popular deep learning libraries ‚Äî Jax, PyTorch, and TensorFlow ‚Äî with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "question = \"Which deep learning libraries back Hugging Face Transformers?\"\n",
    "\n",
    "question_answerer(question=question, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a2d5778-2431-4f2d-8528-572a777acadc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.98023921251297,\n",
       " 'start': 1892,\n",
       " 'end': 1919,\n",
       " 'answer': 'Jax, PyTorch and TensorFlow'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It can deal with very long context\n",
    "long_context = \"\"\"\n",
    "ü§ó Transformers: State of the Art NLP\n",
    "\n",
    "ü§ó Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,\n",
    "question answering, summarization, translation, text generation and more in over 100 languages.\n",
    "Its aim is to make cutting-edge NLP easier to use for everyone.\n",
    "\n",
    "ü§ó Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and\n",
    "then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and\n",
    "can be modified to enable quick research experiments.\n",
    "\n",
    "Why should I use transformers?\n",
    "\n",
    "1. Easy-to-use state-of-the-art models:\n",
    "  - High performance on NLU and NLG tasks.\n",
    "  - Low barrier to entry for educators and practitioners.\n",
    "  - Few user-facing abstractions with just three classes to learn.\n",
    "  - A unified API for using all our pretrained models.\n",
    "  - Lower compute costs, smaller carbon footprint:\n",
    "\n",
    "2. Researchers can share trained models instead of always retraining.\n",
    "  - Practitioners can reduce compute time and production costs.\n",
    "  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.\n",
    "\n",
    "3. Choose the right framework for every part of a model's lifetime:\n",
    "  - Train state-of-the-art models in 3 lines of code.\n",
    "  - Move a single model between TF2.0/PyTorch frameworks at will.\n",
    "  - Seamlessly pick the right framework for training, evaluation and production.\n",
    "\n",
    "4. Easily customize a model or an example to your needs:\n",
    "  - We provide examples for each architecture to reproduce the results published by its original authors.\n",
    "  - Model internals are exposed as consistently as possible.\n",
    "  - Model files can be used independently of the library for quick experiments.\n",
    "\n",
    "ü§ó Transformers is backed by the three most popular deep learning libraries ‚Äî Jax, PyTorch and TensorFlow ‚Äî with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "\n",
    "question_answerer(question=question, context=long_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de03546d-2bb5-49da-b1f6-e09da7b20089",
   "metadata": {},
   "source": [
    "## Step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba32e25f-7fec-4ec4-bf8f-44032362dac8",
   "metadata": {},
   "source": [
    "### Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85baa421-2d37-43da-b29e-823ba9b3527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "model_checkpoint = 'distilbert-base-cased-distilled-squad'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "inputs = tokenizer(question, context, return_tensors='pt')\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab2f3af2-8b52-4399-a8c6-bda3d5e5cc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Which deep learning libraries back Hugging Face Transformers? [SEP] Hugging Face Transformers is backed by the three most popular deep learning libraries ‚Äî Jax, PyTorch, and TensorFlow ‚Äî with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other. [SEP]\n",
      "torch.Size([1, 71]) torch.Size([1, 71])\n"
     ]
    }
   ],
   "source": [
    "# The tokenizer concatenate Q and C: [CLS] question [SEP] context [SEP]\n",
    "print(tokenizer.decode(inputs['input_ids'].flatten().tolist()))\n",
    "\n",
    "# The answer is the logits of the index of start and end for the answer in the original text\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "print(start_logits.shape, end_logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92491da-50b0-43fc-9fbd-00f0fb5c5f24",
   "metadata": {},
   "source": [
    "### Softmax and mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89216429-ba31-40d1-b83c-171696ebb6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n",
      "[False, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True]\n"
     ]
    }
   ],
   "source": [
    "# The softmax step: we need to mask the question and [SEP] in the tokens, but keep [CLS]\n",
    "# Use the sequence_ids from inputs to do the mask\n",
    "\n",
    "import torch\n",
    "\n",
    "sequence_ids = inputs.sequence_ids()\n",
    "mask = [i != 1 for i in sequence_ids]\n",
    "mask[0] = False\n",
    "print(sequence_ids)\n",
    "print(mask)\n",
    "mask = torch.tensor(mask)[None]  # [None] adds a dimension, equivalent to equivalent to n.unsqueeze(dim=1)\n",
    "\n",
    "# Since using softmax next, just give a large neg number\n",
    "start_logits[mask] = -10000\n",
    "end_logits[mask] = -10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa528f63-613b-4ab2-b395-4aee4e96fcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_proba = torch.nn.functional.softmax(start_logits, dim=-1)[0]\n",
    "end_proba = torch.nn.functional.softmax(end_logits, dim=-1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d320e978-20aa-4b9f-822f-8a0cebd31c6e",
   "metadata": {},
   "source": [
    "### Get the start, end indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de671d15-8f71-48db-aae0-f87da17d2ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the events ‚ÄúThe answer starts at start_index‚Äù and ‚ÄúThe answer ends at end_index‚Äù to be\n",
    "# independent, the probability that the answer starts at start_index and ends at end_index is:\n",
    "scores = start_proba[:, None] * end_proba[None, :] \n",
    "# or start_proba.unsqueeze(dim=1) * end_proba.unsqueeze(dim=0)\n",
    "\n",
    "# Mask where start_indx > end_index, i.e. the lower triangular half of the maxtrix. \n",
    "# We use torch.triu to get the upper triangular half only\n",
    "socres = torch.triu(scores)\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import seaborn as sns\n",
    "# df = pd.DataFrame(scores.detach().numpy()) == 0\n",
    "# sns.heatmap(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e323f794-667c-4935-a641-61d1e9ec8ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1956 27 39 0.9779755473136902\n"
     ]
    }
   ],
   "source": [
    "max_index = scores.argmax().item()\n",
    "# or top 3:  torch.topk(scores.flatten(), 3)\n",
    "\n",
    "# Row and column \n",
    "start_index = max_index // scores.shape[0]\n",
    "end_index = max_index % scores.shape[0]\n",
    "print(max_index, start_index, end_index, scores[start_index, end_index].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f439966a-f482-45a1-88b6-dcd9873619e8",
   "metadata": {},
   "source": [
    "### Map to the original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4c7e98bb-f707-4f24-8dff-92013f6c83a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 5), (6, 10), (11, 19), (20, 29), (30, 34), (35, 37), (37, 42), (43, 47), (48, 60), (60, 61), (0, 0), (1, 3), (3, 8), (9, 13), (14, 26), (27, 29), (30, 36), (37, 39), (40, 43), (44, 49), (50, 54), (55, 62), (63, 67), (68, 76), (77, 86), (87, 88), (89, 92), (92, 93), (94, 95), (95, 96), (96, 97), (97, 99), (99, 101), (101, 102), (103, 106), (107, 110), (110, 113), (113, 114), (114, 117), (118, 119), (120, 124), (125, 126), (127, 130), (130, 131), (131, 135), (136, 147), (148, 155), (156, 160), (160, 161), (162, 164), (164, 165), (165, 166), (167, 182), (183, 185), (186, 191), (192, 196), (197, 203), (204, 208), (209, 212), (213, 219), (220, 227), (228, 232), (233, 236), (237, 239), (239, 246), (247, 251), (252, 255), (256, 261), (261, 262), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "# Method 1 - get the mapping to letters\n",
    "inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=True)\n",
    "# The offset_mapping provide mapping from tokens to question+context\n",
    "offsets = inputs_with_offsets['offset_mapping']\n",
    "# Note there are two (0, 0)s\n",
    "print(offsets)\n",
    "\n",
    "# Get index of the original text\n",
    "start_char, _ = offsets[start_index]\n",
    "_, end_char = offsets[end_index]\n",
    "answer = context[start_char:end_char]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b50b9de9-d71d-4ae4-8495-b119d60f9245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jax, PyTorch, and TensorFlow'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 2 - Use input ids directly\n",
    "answer = tokenizer.decode(inputs['input_ids'].flatten()[start_index:end_index+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a1e7a9b0-326e-4f1f-b43f-7e5bac4261a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'Jax, PyTorch, and TensorFlow', 'start': 89, 'end': 117, 'score': tensor(0.9780, grad_fn=<SelectBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "result = {\n",
    "    \"answer\": answer,\n",
    "    \"start\": start_char,\n",
    "    \"end\": end_char,\n",
    "    \"score\": scores[start_index, end_index],\n",
    "}\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731270c4-1931-4217-ac21-c39d90a1f2b8",
   "metadata": {},
   "source": [
    "## Handling long contexts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ac3ea03f-1e5c-4b84-8a8c-cd0272ab90ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463\n"
     ]
    }
   ],
   "source": [
    "max_length = 384  # maximum length of each context allowed in the question-answering pipeline\n",
    "stride = 128  # default stride in the question-answering pipeline. See below\n",
    "inputs = tokenizer(question, long_context)\n",
    "print(len(inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "82825112-e714-4ed0-84f1-4fb38ae9591f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Which deep learning libraries back Hugging Face Transformers? [SEP] [UNK] Transformers : State of the Art NLP [UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation and more in over 100 languages. Its aim is to make cutting - edge NLP easier to use for everyone. [UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine - tune them on your own datasets and then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments. Why should I use transformers? 1. Easy - to - use state - of - the - art models : - High performance on NLU and NLG tasks. - Low barrier to entry for educators and practitioners. - Few user - facing abstractions with just three classes to learn. - A unified API for using all our pretrained models. - Lower compute costs, smaller carbon footprint : 2. Researchers can share trained models instead of always retraining. - Practitioners can reduce compute time and production costs. - Dozens of architectures with over 10, 000 pretrained models, some in more than 100 languages. 3. Choose the right framework for every part of a model's lifetime : - Train state - of - the - art models in 3 lines of code. - Move a single model between TF2. 0 / PyTorch frameworks at will. - Seamlessly pick the right framework for training, evaluation and production. 4. Easily customize a model or an example to your needs : - We provide examples for each architecture to reproduce the results published by its original authors. - [SEP]\n"
     ]
    }
   ],
   "source": [
    "# Method 1 - truncate the context\n",
    "# It's possible that the answer is toward the end of the context and thus was missing\n",
    "inputs = tokenizer(question, long_context, max_length=384, truncation=\"only_second\")\n",
    "print(tokenizer.decode(inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9a2e4892-6b90-4e66-b5d1-ffa9c4c8ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2 - GOOD\n",
    "# Truncate the context by splitting it into chunks with overlaps in case the answer was split\n",
    "sentences = [\n",
    "    \"This sentence is not too long but we are going to split it anyway.\",\n",
    "    \"This sentence is shorter but will still get split.\",\n",
    "]\n",
    "inputs = tokenizer(\n",
    "    sentences, \n",
    "    truncation=True, \n",
    "    return_overflowing_tokens=True,  # to split\n",
    "    max_length=6,   # length of each chunk including marks\n",
    "    stride=2,  # length of overlap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c7295c02-e9fd-42ca-bb7d-4e8cc542deb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] This sentence is not [SEP]\n",
      "[CLS] is not too long [SEP]\n",
      "[CLS] too long but we [SEP]\n",
      "[CLS] but we are going [SEP]\n",
      "[CLS] are going to split [SEP]\n",
      "[CLS] to split it anyway [SEP]\n",
      "[CLS] it anyway. [SEP]\n",
      "[CLS] This sentence is shorter [SEP]\n",
      "[CLS] is shorter but will [SEP]\n",
      "[CLS] but will still get [SEP]\n",
      "[CLS] still get split. [SEP]\n"
     ]
    }
   ],
   "source": [
    "for ids in inputs['input_ids']:\n",
    "    print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e3f42f39-8a23-47b1-89ef-7d5cefc6f432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# which sentence each of the results corresponds to\n",
    "print(inputs['overflow_to_sample_mapping'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b675fd0e-3ecd-44fc-86b3-5d73aa1ee065",
   "metadata": {},
   "source": [
    "### Step by step - with long_context split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1d036f35-29bb-440e-9fcd-fd991caf35ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    question,\n",
    "    long_context,\n",
    "    stride=stride,\n",
    "    max_length=max_length,\n",
    "    padding=\"longest\",\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "563b9cd0-cfaa-4ed8-a178-670012b72a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 384])\n"
     ]
    }
   ],
   "source": [
    "# Remove fields not useful for the model\n",
    "_ = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "offsets = inputs.pop('offset_mapping')\n",
    "\n",
    "inputs = inputs.convert_to_tensors('pt')\n",
    "print(inputs['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5be9baf6-d25e-464f-b018-23f0904f4f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 384]) torch.Size([2, 384])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "print(start_logits.shape, end_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "64aa9dc8-4892-47d2-94d1-b9c8104f80c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_ids = inputs.sequence_ids()\n",
    "mask = [i != 1 for i in sequence_ids]\n",
    "# Unmask [CLS]\n",
    "mask[0] = False\n",
    "# Mask all the [PAD] tokens\n",
    "mask = torch.logical_or(torch.tensor(mask)[None], (inputs['attention_mask'] == 0))\n",
    "\n",
    "start_logits[mask] = -10000\n",
    "end_logits[mask] = -10000\n",
    "\n",
    "start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)\n",
    "end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c7f44807-a13e-4d1e-a7ee-5158cf24f59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0, 0.867302656173706), (177, 188, 0.9802390933036804)]\n",
      "{'answer': '[CLS]', 'score': 0.867302656173706}\n",
      "{'answer': 'Jax, PyTorch and TensorFlow', 'score': 0.9802390933036804}\n"
     ]
    }
   ],
   "source": [
    "# Get the best score and answer\n",
    "# Since there are multiple chunks of context, we need to go over each one\n",
    "candidates = []\n",
    "for start_probs, end_probs in zip(start_probabilities, end_probabilities):\n",
    "    scores = start_probs[:, None] * end_probs[None, :]\n",
    "    idx = torch.triu(scores).argmax().item()\n",
    "\n",
    "    start_idx = idx // scores.shape[0]\n",
    "    end_idx = idx % scores.shape[0]\n",
    "    score = scores[start_idx, end_idx].item()\n",
    "    candidates.append((start_idx, end_idx, score))\n",
    "print(candidates)\n",
    "\n",
    "for candidate, input_ids in zip(candidates, inputs['input_ids']):\n",
    "    start_token, end_token, score = candidate\n",
    "    answer = tokenizer.decode(input_ids[start_token:end_token+1])\n",
    "    result = {'answer': answer, 'score': score}    \n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "1a6e8301-780a-4e09-8ced-4f9ae4d21c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 0, 0.867302656173706), (0, 187, 0.0064314608462154865)], [(177, 188, 0.9802390933036804), (177, 189, 0.013365774415433407)]]\n",
      "{'answer': '[CLS]', 'score': 0.867302656173706}\n",
      "{'answer': '[CLS] Which deep learning libraries back Hugging Face Transformers? [SEP] [UNK] Transformers : State of the Art NLP [UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation and more in over 100 languages. Its aim is to make cutting - edge NLP easier to use for everyone. [UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine - tune them on your own datasets and then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments. Why should I use transformers? 1. Easy - to - use state - of - the - art models : - High performance on NLU and NLG tasks. - Low barrier to entry for educators and practitioners', 'score': 0.0064314608462154865}\n",
      "{'answer': 'Jax, PyTorch and TensorFlow', 'score': 0.9802390933036804}\n",
      "{'answer': 'Jax, PyTorch and TensorFlow ‚Äî', 'score': 0.013365774415433407}\n"
     ]
    }
   ],
   "source": [
    "# Get the top 2 scores and answers\n",
    "candidates_topk = []\n",
    "for start_probs, end_probs in zip(start_probabilities, end_probabilities):\n",
    "    candidates = []\n",
    "    scores = torch.triu(start_probs[:, None] * end_probs[None, :])\n",
    "    idx_topk = scores.flatten().topk(2)\n",
    "    for score, idx in zip(idx_topk[0], idx_topk[1]):\n",
    "        idx = idx.item()\n",
    "        start_idx = idx // scores.shape[0]\n",
    "        end_idx = idx % scores.shape[0]\n",
    "        candidates.append((start_idx, end_idx, score.item()))\n",
    "    candidates_topk.append(candidates)\n",
    "print(candidates_topk)\n",
    "\n",
    "for candidates, input_ids in zip(candidates_topk, inputs['input_ids']):\n",
    "    for candidate in candidates:\n",
    "        start_token, end_token, score = candidate\n",
    "        answer = tokenizer.decode(input_ids[start_token:end_token+1])\n",
    "        result = {'answer': answer, 'score': score}    \n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b51da0-9767-41e8-9e58-8907e33e861c",
   "metadata": {},
   "source": [
    "# Next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb549b57-105d-40d4-a81d-ac889848551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Next: https://huggingface.co/learn/nlp-course/chapter6/4?fw=pt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
