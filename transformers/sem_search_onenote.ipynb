{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Semantic search with sentence embedding\n",
    "Search OneNote page and paragraph best matching a given query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Code\\env\\penv\\Lib\\site-packages\\transformers\\utils\\hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "d:\\Code\\env\\penv\\Lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "d:\\Code\\env\\penv\\Lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "from sem_search_pdf.utils.embedding_generator import generate_embeddings_for_dataframe\n",
    "from sem_search_pdf.utils.pdf_reader import extract_information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/test.pdf'\n",
    "file_name = 'test.pdf'\n",
    "tmp = extract_information(file_path, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create embeddings\n",
    "\n",
    "Instructions: https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/new-impl:\n",
      "- configuration.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/new-impl:\n",
      "- modeling.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A matching Triton is not available, some optimizations will not be enabled.\n",
      "Error caught was: No module named 'triton'\n",
      "Some weights of the model checkpoint at Alibaba-NLP/gte-multilingual-base were not used when initializing NewModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NewModel(\n",
       "  (embeddings): NewEmbeddings(\n",
       "    (word_embeddings): Embedding(250048, 768, padding_idx=1)\n",
       "    (rotary_emb): NTKScalingRotaryEmbedding()\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): NewEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x NewLayer(\n",
       "        (attention): NewAttention(\n",
       "          (qkv_proj): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (mlp): NewGatedMLP(\n",
       "          (up_gate_proj): Linear(in_features=768, out_features=6144, bias=False)\n",
       "          (down_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act_fn): GELUActivation()\n",
       "          (hidden_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (mlp_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (hidden_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ckpt = 'Alibaba-NLP/gte-multilingual-base'  # \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt, trust_remote_code=True)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66fb8a1018844cf1b5efd088562a2590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/262 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Code\\env\\penv\\Lib\\site-packages\\dill\\_dill.py:412: PicklingWarning: Cannot locate reference to <class 'transformers_modules.Alibaba-NLP.new-impl.40ced75c3017eb27626c9d4ea981bde21a2662f4.modeling.NewAttention'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "d:\\Code\\env\\penv\\Lib\\site-packages\\dill\\_dill.py:412: PicklingWarning: Cannot pickle <class 'transformers_modules.Alibaba-NLP.new-impl.40ced75c3017eb27626c9d4ea981bde21a2662f4.modeling.NewAttention'>: transformers_modules.Alibaba-NLP.new-impl.40ced75c3017eb27626c9d4ea981bde21a2662f4.modeling.NewAttention has recursive self-references that trigger a RecursionError.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "d:\\Code\\env\\penv\\Lib\\site-packages\\dill\\_dill.py:412: PicklingWarning: Cannot locate reference to <class 'transformers_modules.Alibaba-NLP.new-impl.40ced75c3017eb27626c9d4ea981bde21a2662f4.modeling.IndexFirstAxis'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "d:\\Code\\env\\penv\\Lib\\site-packages\\dill\\_dill.py:412: PicklingWarning: Cannot pickle <class 'transformers_modules.Alibaba-NLP.new-impl.40ced75c3017eb27626c9d4ea981bde21a2662f4.modeling.IndexFirstAxis'>: transformers_modules.Alibaba-NLP.new-impl.40ced75c3017eb27626c9d4ea981bde21a2662f4.modeling.IndexFirstAxis has recursive self-references that trigger a RecursionError.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "d:\\Code\\env\\penv\\Lib\\site-packages\\dill\\_dill.py:412: PicklingWarning: Cannot locate reference to <class 'transformers_modules.Alibaba-NLP.new-impl.40ced75c3017eb27626c9d4ea981bde21a2662f4.modeling.IndexPutFirstAxis'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "d:\\Code\\env\\penv\\Lib\\site-packages\\dill\\_dill.py:412: PicklingWarning: Cannot pickle <class 'transformers_modules.Alibaba-NLP.new-impl.40ced75c3017eb27626c9d4ea981bde21a2662f4.modeling.IndexPutFirstAxis'>: transformers_modules.Alibaba-NLP.new-impl.40ced75c3017eb27626c9d4ea981bde21a2662f4.modeling.IndexPutFirstAxis has recursive self-references that trigger a RecursionError.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b79134ea9bef438d9875901ff21d7181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/262 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tt = generate_embeddings_for_dataframe(tmp, tokenizer, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dcdc17bbcec4131ad12f7c9c275fb9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['page_in_on', 'title', 'page_in_pdf', 'paragraph', 'text', 'file_name', 'data', 'embeddings'],\n",
       "    num_rows: 262\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install faiss-cpu\n",
    "tt.add_faiss_index(column=\"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sem_search_pdf.utils.embedding_generator import compute_embeddings\n",
    "question = \"How to use prompt to create a wiki\"\n",
    "question_embedding = compute_embeddings([question], tokenizer, model, device).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, samples = tt.get_nearest_examples(\n",
    "    \"embeddings\", question_embedding, k=5\n",
    ")\n",
    "import pandas as pd\n",
    "samples_df = pd.DataFrame.from_dict(samples)\n",
    "samples_df[\"scores\"] = scores\n",
    "samples_df.sort_values(\"scores\", ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAGE: 9\n",
      "PAGE IN PDF: 19\n",
      "PARAGRAGH IN PDF: 4\n",
      "SCORE: 292.341796875\n",
      "TITLE: Prompt Engineering\n",
      "TEXT:  We will continue this iterative process with me \n",
      "providing additional information to you and you updating \n",
      "the prompt in the Revised prompt section until it's \n",
      "complete.\n",
      "A Chinse \n",
      "version\n",
      "我想请你xxxxx，请问我应该如何向你提问才能得到最满意的答\n",
      "案，请提供全面、详细的建议，针对每一个建议请你提供具体\n",
      "的提问范例，注意这些范例都是关于如何向你提问xxxxx，最后\n",
      "根据你所有的建议，再综合提供一个总的提问范例，注意这个\n",
      "范例必须同时体现你所有的建议。\n",
      "Another \n",
      "English version \n",
      "in case of \n",
      "misunderstandi\n",
      "ng \n",
      "Rephrase and expand the question, and respond\n",
      "Wiki 获取信息,学习领\n",
      "域,术语等\n",
      "I want you to act as a Wikipedia page. I will give you the \n",
      "name of a topic, and you will provide a summary of that \n",
      "topic in the format of a Wikipedia page. Your summary \n",
      "should be informative and factual, covering the most \n",
      "important aspects of the topic. Start your summary with an \n",
      "introductory paragraph that gives an overview of the topic. \n",
      "My first topic is “The Great Barrier Reef.”\n",
      "General 定义角色和任务\n",
      "\n",
      "==================================================\n",
      "\n",
      "PAGE: 9\n",
      "PAGE IN PDF: 19\n",
      "PARAGRAGH IN PDF: 1\n",
      "SCORE: 304.4001770019531\n",
      "TITLE: Prompt Engineering\n",
      "TEXT: Name What Prompt_English Prompt_Chinese\n",
      "Let \n",
      "ChatGPT \n",
      "improve \n",
      "my prompt\n",
      "任务相对复杂，\n",
      "而自己经验或专\n",
      "业知识比较少，\n",
      "导致无法准确描\n",
      "述需求的情况\n",
      "I want you to become my Prompt Creator. Your goal is to \n",
      "help me craft the best possible prompt for my needs. The \n",
      "prompt will be used by you, ChatGPT. You will follow the \n",
      "following process: \n",
      "\n",
      "==================================================\n",
      "\n",
      "PAGE: 9\n",
      "PAGE IN PDF: 19\n",
      "PARAGRAGH IN PDF: 10\n",
      "SCORE: 323.9619140625\n",
      "TITLE: Prompt Engineering\n",
      "TEXT: 确认启动：告诉ChatGPT培训期结束，已经上岗工作了，用于明确启动指令。prompt格式如下下面我将把我的需求发给你，请严格按照上述规范输出答案。\n",
      "\n",
      "==================================================\n",
      "\n",
      "PAGE: 9\n",
      "PAGE IN PDF: 19\n",
      "PARAGRAGH IN PDF: 5\n",
      "SCORE: 328.05096435546875\n",
      "TITLE: Prompt Engineering\n",
      "TEXT: 明确规范2.\n",
      "投喂示例3.\n",
      "确认启动4.\n",
      "纠错调优5.\n",
      "定义角色和任务：告诉ChatGPT应该担任什么角色或者岗位，和需要完成什么任务。prompt格式如下我希望你担任一个专业的XXX，来帮我完成XXX。\n",
      "\n",
      "==================================================\n",
      "\n",
      "PAGE: 9\n",
      "PAGE IN PDF: 19\n",
      "PARAGRAGH IN PDF: 9\n",
      "SCORE: 330.33526611328125\n",
      "TITLE: Prompt Engineering\n",
      "TEXT: 投喂示例：根据上个对话的规范，给ChatGPT投喂对应的优秀示例让它进一步学习规范。示例可以多投喂几个。prompt\n",
      "格式如下以下是几个优秀的示例供你参考，请你理解需求和输出答案的时候参考示例【示例一】XXX\n",
      "【示例二】XXX\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _, row in samples_df.iterrows():\n",
    "    print(f\"PAGE: {row.page_in_on}\")\n",
    "    print(f\"PAGE IN PDF: {row.page_in_pdf}\")\n",
    "    print(f\"PARAGRAGH IN PDF: {row.paragraph}\")\n",
    "    print(f\"SCORE: {row.scores}\")\n",
    "    print(f\"TITLE: {row.title}\")\n",
    "    print(f\"TEXT: {row.text}\")\n",
    "    print(\"=\" * 50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预训练阶段，模型使用和 GPT-3 相同的数据集进行无监督学习，学习语言的基本知识和规律。 -----\n",
      "\n",
      "微调阶段，模型使用一些人工标注的数据进行强化学习，学习如何根据指令生成合适的输出。\n",
      "收集人类反馈:使用初始化模型生成多个不同摘要人工进行排序，得到一批排好序的摘要样本;\n",
      "人工标注的数据包括两部分：指令和反馈。指令是一些用自然语言描述的任务，如 “写一首关于春天的诗” 或 “给我一个关于狗的笑话”。反馈是一些用数字表示的评分，如 “1” 表示很差，\n",
      "“5” 表示很好。反馈是由人类标注者根据模型的输出给出的，反映了模型输出的质量和合理性。\n",
      "1)\n",
      " -----\n",
      "\n",
      "训练奖励模型:使用第1步得到的样本集，训练模型.该模型输入为一篇文章和对应的一个摘要，模型输出为该摘要的得分ii.\n",
      "训练策略模型:使用初始化的策略模型生成一篇文章的摘要，然后使用奖励模型对该摘要打分，再使用打分值借助PPO算法重新优化策略模型;iii.\n",
      " -----\n",
      "\n",
      "(在微调阶段，模型使用一个叫做 Actor-Critic 的算法进行强化学习。Actor-Critic 算法包括两个部分：Actor 和 Critic。Actor 是一个生成器，它根据指令生成输出。Critic 是一个评估器，它根据反馈评估\n",
      "输出的奖励值。Actor 和 Critic 之间相互协作和竞争，不断地更新自己的参数，以提高奖励值。)\n",
      " -----\n",
      "\n",
      "ii.\n",
      "技术细节iii.\n",
      "Step The Supervised Fine-Tuning (SFT) policyThe reward model (RM) Reinforcement Learning\n",
      "Goal Collect demonstration data to train the \n",
      "SFT model to learn the \"proper\" \n",
      "response to a prompt\n",
      "Learn an objective function directly from the data. Build an automatic \n",
      "system to mimic human preferences.\n",
      "The purpose of this function is togive a score to the SFT model outputs \n",
      "to reflect human preferencesand ethics of model outputs/responses. In \n",
      "the end, this process will extract from the data an automatic system that \n",
      "is supposed to mimic human preferences.\n",
      "Fine-tuning the SFT model via Proximal Policy Optimization (PPO) to \n",
      "optimize the reward model\n",
      "Method Data collection - prompt-response pairs\n",
      "A list of prompts is selected either \n",
      "directly from labelers, or sampled \n",
      "from customers prompts from \n",
      "previous models (GPT-3)\n",
      " -----\n",
      "\n",
      "A group of human labelers are \n",
      "asked to write down the expected \n",
      "output response. \n",
      " -----\n",
      "\n",
      "This results in is a relatively small, \n",
      "high-quality curated dataset (of \n",
      "approximately 12-15k data points, \n",
      "presumably) to fine-tune a \n",
      "pretrained language model.\n",
      " -----\n",
      "\n",
      "Train the model with the data\n",
      "Choice of model: the baseline \n",
      "model used is the latest one text-\n",
      "davinci-003, a GPT-3 model which \n",
      "was fine-tuned mostly on \n",
      "programming code (a code model, \n",
      "rather a pure text model)\n",
      " -----\n",
      "\n",
      "Labelers rank the outputs from best to worst to create a reward model \n",
      "A list of prompts is selected and the SFT model generates multiple \n",
      "outputs for each prompt.\n",
      " -----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for i in range(9, 18):\n",
    "#     # print(df[(df.page_in_pdf == 17) & (df.paragraph == i)].text.values[0], '-----\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 2\n",
    "Refer to https://huggingface.co/docs/transformers/tasks/language_modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8653533af8f044d9a6160d4f968cbaca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75f0a90bd7a0470c8fc9219b26eeb341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Not enough memory on my Legion Y9000P. Need to set pagefile to system managed\n",
    "from transformers import pipeline\n",
    "checkpoint = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "generator = pipeline(\"text-generation\", model=checkpoint, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3min 4s\n",
      "Wall time: 8min 26s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': \"How much memory is needed to run Llama2 \\n\\nAnswer: Llama2 is a relatively lightweight library, and it doesn't require a lot of memory to run. In fact, Llama2 is designed Limited spin faut pap Fürulen mű efectспе Welcome以 politique domin\"}]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ['How much memory is needed to run Llama2 ']\n",
    "%time response = generator(prompt, max_new_tokens=50, num_beams=2, do_sample=True, top_k=5, top_p=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'checkpoint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[1;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[43mcheckpoint\u001b[49m)\n\u001b[0;32m      4\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n\u001b[0;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(checkpoint)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'checkpoint' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(raw_inputs, \n",
    "                   # padding='longest', truncation=True, max_length=128, \n",
    "                   return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(inputs, max_new_tokens=20, num_beams=2, do_sample=True, top_k=5, top_p=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Hello. Who are you?\\n\\nComment: Hello! I'm just an AI designed to assist and communicate with users\"]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "checkpoint = 'codellama/CodeLlama-7b-Python-hf'\n",
    "# generator = pipeline(\"text-generation\", model=checkpoint, device_map='auto')\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint,\n",
    "    # load_in_8bit=True,\n",
    "    # torch_dtype=torch.float16,\n",
    "    offload_folder=\"./save_folder\",  # Need to create this folder anyway\n",
    "    device_map=\"auto\",\n",
    "    # device_map={\"\": 0},  # not enough GPU memory\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\")\n",
    "\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.add_eos_token = True\n",
    "# tokenizer.pad_token_id = 0\n",
    "# tokenizer.padding_side = \"left\"\n",
    "\n",
    "prompt = 'Write a piece of Python code to order a list of numbers'\n",
    "inputs = tokenizer(prompt, \n",
    "                   # padding='longest', truncation=True, max_length=128, \n",
    "                   return_tensors=\"pt\"\n",
    "                  )\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**inputs, max_new_tokens=100)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ['Write a piece of Python code to order a list of numbers']\n",
    "%time generator(prompt, max_new_tokens=50, num_beams=2, do_sample=True, top_k=5, top_p=0.95)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
