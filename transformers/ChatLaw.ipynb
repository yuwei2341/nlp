{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "qmsIKj90k8St"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5faf0aa7ac2b4ff296a1e18fe124f09b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_133b59ce9a304b19898225116599be17",
              "IPY_MODEL_78772e872408443ba1131751604f41e6",
              "IPY_MODEL_e82eee781c7f46899329768717e6d731"
            ],
            "layout": "IPY_MODEL_28e41ef38b4240a08a06e72f5e9bb931"
          }
        },
        "133b59ce9a304b19898225116599be17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c437fce21505436bb6f712f6815ecfd2",
            "placeholder": "​",
            "style": "IPY_MODEL_174b25aabb134fcc8d9fc0d6ba7486cc",
            "value": "Fetching 51 files: 100%"
          }
        },
        "78772e872408443ba1131751604f41e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24f1a54058ef45f9803729790f526b5c",
            "max": 51,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_33899c6a4f064878ab3cb52966b837dc",
            "value": 51
          }
        },
        "e82eee781c7f46899329768717e6d731": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a21609d0d5d4199b8e60ba8123d2774",
            "placeholder": "​",
            "style": "IPY_MODEL_bc7f2ed3ac504f02a10281fc6c0da79e",
            "value": " 51/51 [00:00&lt;00:00, 40.70it/s]"
          }
        },
        "28e41ef38b4240a08a06e72f5e9bb931": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c437fce21505436bb6f712f6815ecfd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "174b25aabb134fcc8d9fc0d6ba7486cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24f1a54058ef45f9803729790f526b5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33899c6a4f064878ab3cb52966b837dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1a21609d0d5d4199b8e60ba8123d2774": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc7f2ed3ac504f02a10281fc6c0da79e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "lzIk2N_olG2-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eh3E5_5vl0pG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "736128ff-5254-43c3-86f2-b1f7c453d6ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers[sentencepiece]\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers[sentencepiece])\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers[sentencepiece])\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers[sentencepiece])\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (4.65.0)\n",
            "Collecting sentencepiece!=0.1.92,>=0.1.91 (from transformers[sentencepiece])\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf<=3.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[sentencepiece]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[sentencepiece]) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.4)\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.30.2\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.13.1-py3-none-any.whl (486 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.13.1 dill-0.3.6 multiprocess-0.70.14 xxhash-3.2.0\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.20.3\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.22.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.1)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2022.7.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: responses, evaluate\n",
            "Successfully installed evaluate-0.4.0 responses-0.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers[sentencepiece]\n",
        "!pip install datasets\n",
        "!pip install accelerate -U\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import file_utils\n",
        "print(file_utils.default_cache_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5oXUrLQmLiC",
        "outputId": "c97c73ee-2587-4150-ea75-44190768ab01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/.cache/huggingface/hub\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!ls /root/.cache/huggingface/hub/\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBssYVKi4Sl9",
        "outputId": "3c54e45c-d57f-40f4-cafd-15bc38e6a47e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models--decapoda-research--llama-13b-hf  tmpm7zr4vs8  tmpsqlm86mi  tmpwfqhimid\n",
            "tmp295klh7g\t\t\t\t tmpnpmnh996  tmpsu6n8zq9  tmpzd9sfgy2\n",
            "tmp4x03vmlk\t\t\t\t tmprceq3_7r  tmpv4pq7am7  version.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download base and delta\n",
        "\n",
        "from huggingface_hub import snapshot_download\n",
        "checkpoint_delta = \"IDEA-CCNL/Ziya-LLaMA-13B-v1\"\n",
        "checkpoint_base = \"decapoda-research/llama-13b-hf\"\n",
        "\n",
        "snapshot_download(repo_id=checkpoint_base)\n",
        "# snapshot_download(repo_id=checkpoint_delta)"
      ],
      "metadata": {
        "id": "tdIxV0F8mUeH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68,
          "referenced_widgets": [
            "5faf0aa7ac2b4ff296a1e18fe124f09b",
            "133b59ce9a304b19898225116599be17",
            "78772e872408443ba1131751604f41e6",
            "e82eee781c7f46899329768717e6d731",
            "28e41ef38b4240a08a06e72f5e9bb931",
            "c437fce21505436bb6f712f6815ecfd2",
            "174b25aabb134fcc8d9fc0d6ba7486cc",
            "24f1a54058ef45f9803729790f526b5c",
            "33899c6a4f064878ab3cb52966b837dc",
            "1a21609d0d5d4199b8e60ba8123d2774",
            "bc7f2ed3ac504f02a10281fc6c0da79e"
          ]
        },
        "outputId": "a0cfa0f4-f47f-4e86-f6be-43ff45e2c8e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 51 files:   0%|          | 0/51 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5faf0aa7ac2b4ff296a1e18fe124f09b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/root/.cache/huggingface/hub/models--decapoda-research--llama-13b-hf/snapshots/438770a656712a5072229b62256521845d4de5ce'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/IDEA-CCNL/Fengshenbang-LM/blob/main/fengshen/utils/apply_delta.py ."
      ],
      "metadata": {
        "id": "Dmz7b5IMmb0H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a77ddc5-489b-46e3-9a24-4e79c864a447"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-11 01:37:30--  https://github.com/IDEA-CCNL/Fengshenbang-LM/blob/main/fengshen/utils/apply_delta.py\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 36904 (36K) [text/plain]\n",
            "Saving to: ‘apply_delta.py’\n",
            "\n",
            "apply_delta.py      100%[===================>]  36.04K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-07-11 01:37:31 (2.99 MB/s) - ‘apply_delta.py’ saved [36904/36904]\n",
            "\n",
            "--2023-07-11 01:37:31--  http://./\n",
            "Resolving . (.)... failed: No address associated with hostname.\n",
            "wget: unable to resolve host address ‘.’\n",
            "FINISHED --2023-07-11 01:37:31--\n",
            "Total wall clock time: 0.4s\n",
            "Downloaded: 1 files, 36K in 0.01s (2.99 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define paths"
      ],
      "metadata": {
        "id": "0cUgBznplLUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_path = \"/root/.cache/huggingface/hub/models--decapoda-research--llama-13b-hf/snapshots/438770a656712a5072229b62256521845d4de5ce/\"\n",
        "delta_path = \"/root/.cache/huggingface/hub/models--IDEA-CCNL--Ziya-LLaMA-13B-v1/snapshots/fccf34387d2c9f2f95ff59ae380e6de3718e41ff\"\n",
        "target_model_path = '/content/target/tmp/'"
      ],
      "metadata": {
        "id": "lFtUhbhSvnbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_path=base_model_path\n",
        "target_model_path=target_model_path\n",
        "delta_path=delta_path\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RZXh0ZPi7Cq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Original merging code"
      ],
      "metadata": {
        "id": "qmsIKj90k8St"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import gc\n",
        "import glob\n",
        "import json\n",
        "import os\n",
        "import shutil\n",
        "import tempfile\n",
        "\n",
        "from huggingface_hub import snapshot_download\n",
        "import torch\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, LlamaForCausalLM\n",
        "\n",
        "\n",
        "GB = 1 << 30\n",
        "\n",
        "\n",
        "def split_files(model_path, tmp_path, split_size):\n",
        "    if not os.path.exists(model_path):\n",
        "        model_path = snapshot_download(repo_id=model_path)\n",
        "    if not os.path.exists(tmp_path):\n",
        "        os.makedirs(tmp_path)\n",
        "\n",
        "    file_pattern = os.path.join(model_path, \"pytorch_model-*.bin\")\n",
        "    files = glob.glob(file_pattern)\n",
        "\n",
        "    part = 0\n",
        "    try:\n",
        "        for file_path in tqdm(files):\n",
        "            state_dict = torch.load(file_path)\n",
        "            new_state_dict = {}\n",
        "\n",
        "            current_size = 0\n",
        "            for name, param in state_dict.items():\n",
        "                param_size = param.numel() * param.element_size()\n",
        "\n",
        "                if current_size + param_size > split_size:\n",
        "                    new_file_name = f\"pytorch_model-{part}.bin\"\n",
        "                    new_file_path = os.path.join(tmp_path, new_file_name)\n",
        "                    torch.save(new_state_dict, new_file_path)\n",
        "                    current_size = 0\n",
        "                    new_state_dict = None\n",
        "                    gc.collect()\n",
        "                    new_state_dict = {}\n",
        "                    part += 1\n",
        "\n",
        "                new_state_dict[name] = param\n",
        "                current_size += param_size\n",
        "\n",
        "            new_file_name = f\"pytorch_model-{part}.bin\"\n",
        "            new_file_path = os.path.join(tmp_path, new_file_name)\n",
        "            torch.save(new_state_dict, new_file_path)\n",
        "            new_state_dict = None\n",
        "            gc.collect()\n",
        "            new_state_dict = {}\n",
        "            part += 1\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during split_files: {e}\")\n",
        "        shutil.rmtree(tmp_path)\n",
        "        raise\n",
        "\n",
        "\n",
        "def apply_delta_low_cpu_mem(base_model_path, target_model_path, delta_path):\n",
        "    delta_tokenizer = AutoTokenizer.from_pretrained(delta_path, use_fast=False)\n",
        "    delta_config = AutoConfig.from_pretrained(delta_path)\n",
        "\n",
        "    if os.path.exists(target_model_path):\n",
        "        shutil.rmtree(target_model_path)\n",
        "    os.makedirs(target_model_path)\n",
        "\n",
        "    split_size = 4 * GB\n",
        "\n",
        "    with tempfile.TemporaryDirectory() as tmp_base_path, tempfile.TemporaryDirectory() as tmp_delta_path:\n",
        "        print(f\"Split files for the base model to {tmp_base_path}\")\n",
        "        split_files(base_model_path, tmp_base_path, split_size)\n",
        "        print(f\"Split files for the delta weights to {tmp_delta_path}\")\n",
        "        split_files(delta_path, tmp_delta_path, split_size)\n",
        "\n",
        "        base_pattern = os.path.join(tmp_base_path, \"pytorch_model-*.bin\")\n",
        "        base_files = glob.glob(base_pattern)\n",
        "        delta_pattern = os.path.join(tmp_delta_path, \"pytorch_model-*.bin\")\n",
        "        delta_files = glob.glob(delta_pattern)\n",
        "        delta_state_dict = torch.load(delta_files[0])\n",
        "\n",
        "        print(\"Applying the delta\")\n",
        "        weight_map = {}\n",
        "        total_size = 0\n",
        "\n",
        "        for i, base_file in tqdm(enumerate(base_files)):\n",
        "            state_dict = torch.load(base_file)\n",
        "            file_name = f\"pytorch_model-{i}.bin\"\n",
        "            for name, param in state_dict.items():\n",
        "                if name not in delta_state_dict:\n",
        "                    for delta_file in delta_files:\n",
        "                        delta_state_dict = torch.load(delta_file)\n",
        "                        gc.collect()\n",
        "                        if name in delta_state_dict:\n",
        "                            break\n",
        "                if \"embed_tokens\" in name or \"lm_head.weight\" in name or \"self_attn.rotary_emb.inv_freq\" in name:\n",
        "                    state_dict[name] = delta_state_dict[name]\n",
        "                else:\n",
        "                    state_dict[name] += delta_state_dict[name]\n",
        "                weight_map[name] = file_name\n",
        "                total_size += param.numel() * param.element_size()\n",
        "                gc.collect()\n",
        "            torch.save(state_dict, os.path.join(target_model_path, file_name))\n",
        "\n",
        "        with open(\n",
        "            os.path.join(target_model_path, \"pytorch_model.bin.index.json\"), \"w\"\n",
        "        ) as f:\n",
        "            json.dump(\n",
        "                {\"weight_map\": weight_map, \"metadata\": {\"total_size\": total_size}}, f\n",
        "            )\n",
        "\n",
        "    print(f\"Saving the target model to {target_model_path}\")\n",
        "    delta_tokenizer.save_pretrained(target_model_path)\n",
        "    delta_config.save_pretrained(target_model_path)\n",
        "\n",
        "\n",
        "def apply_delta(base_model_path, target_model_path, delta_path):\n",
        "    print(f\"Loading the delta weights from {delta_path}\")\n",
        "    delta_tokenizer = AutoTokenizer.from_pretrained(delta_path,     use_fast=False, local_files_only=True,)\n",
        "    delta = LlamaForCausalLM.from_pretrained(\n",
        "        delta_path, torch_dtype=torch.float16, low_cpu_mem_usage=True, local_files_only=True,\n",
        "    )\n",
        "\n",
        "    print(f\"Loading the base model from {base_model_path}\")\n",
        "    base = LlamaForCausalLM.from_pretrained(\n",
        "        base_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True, local_files_only=True,\n",
        "    )\n",
        "\n",
        "    print(\"Applying the delta\")\n",
        "    for name, param in tqdm(delta.state_dict().items(), desc=\"Applying delta\"):\n",
        "        assert name in base.state_dict()\n",
        "        # param.data += delta.state_dict()[name]\n",
        "        if \"embed_tokens\" in name or \"lm_head.weight\" in name or \"self_attn.rotary_emb.inv_freq\" in name:\n",
        "            continue\n",
        "        else:\n",
        "            param.data += base.state_dict()[name]\n",
        "    # base.model.embed_tokens = delta.model.embed_tokens\n",
        "    # base.lm_head.weight = delta.lm_head.weight\n",
        "\n",
        "    print(f\"Saving the target model to {target_model_path}\")\n",
        "    delta.save_pretrained(target_model_path)\n",
        "    delta_tokenizer.save_pretrained(target_model_path)\n",
        "\n",
        "# !python3 -m apply_delta.py --base /root/.cache/huggingface/hub/models--decapoda-research--llama-7b-hf --target /root/.cache/huggingface/hub/models--IDEA-CCNL--Ziya-LLaMA-13B --delta /root/.cache/huggingface/hub/models--IDEA-CCNL--Ziya-LLaMA-13B-v1\n",
        "# apply_delta_low_cpu_mem(base_model_path=base_model_path,\n",
        "#             target_model_path=\"/root/.cache/huggingface/hub/models--IDEA-CCNL--Ziya-LLaMA-13B-v1/snapshots/tmp/\",\n",
        "#             delta_path=delta_path,\n",
        "#             )"
      ],
      "metadata": {
        "id": "KTT3tsr6yUTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# apply_delta_low_cpu_mem"
      ],
      "metadata": {
        "id": "HQVY3NzpkUVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delta_tokenizer = AutoTokenizer.from_pretrained(delta_path, use_fast=False)\n",
        "delta_config = AutoConfig.from_pretrained(delta_path)\n",
        "\n",
        "if os.path.exists(target_model_path):\n",
        "    shutil.rmtree(target_model_path)\n",
        "os.makedirs(target_model_path)\n",
        "\n",
        "# split_size = 6 * GB\n",
        "split_size = 4 * GB\n",
        "\n",
        "# with tempfile.TemporaryDirectory() as tmp_base_path, tempfile.TemporaryDirectory() as tmp_delta_path:\n",
        "tmp_base_path = '/content/tmp/base'\n",
        "tmp_delta_path = '/content/tmp/delta'\n",
        "\n",
        "# The file is not large. Just copy them\n",
        "# !cp /root/.cache/huggingface/hub/models--IDEA-CCNL--Ziya-LLaMA-13B-v1/snapshots/fccf34387d2c9f2f95ff59ae380e6de3718e41ff/*.bin tmp/delta/\n",
        "# !cp /root/.cache/huggingface/hub/models--decapoda-research--llama-13b-hf/snapshots/438770a656712a5072229b62256521845d4de5ce/*.bin tmp/base/\n",
        "\n",
        "# !rm -rf /root/.cache/huggingface/hub/models--IDEA-CCNL--Ziya-LLaMA-13B-v1/snapshots/fccf34387d2c9f2f95ff59ae380e6de3718e41ff/*.bin\n",
        "# !rm -rf  /root/.cache/huggingface/hub/models--decapoda-research--llama-13b-hf/snapshots/438770a656712a5072229b62256521845d4de5ce/*.bin\n",
        "\n",
        "# This is the original code to split the files which are not necessary\n",
        "# print(f\"Split files for the base model to {tmp_base_path}\")\n",
        "# split_files(base_model_path, tmp_base_path, split_size)\n",
        "# print(f\"Split files for the delta weights to {tmp_delta_path}\")\n",
        "# split_files(delta_path, tmp_delta_path, split_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Y89rhWL694n",
        "outputId": "a6a5228a-2002-4068-f7f5-90cf9faa11e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split files for the base model to /content/tmp/base\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 33/33 [03:18<00:00,  6.00s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "xmQsIRkoJ7Bb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "START FROM HERE\n",
        "# remove some base files and do the merge. and remove and do\n",
        "\n",
        "\n",
        "base_pattern = os.path.join(tmp_base_path, \"pytorch_model-*.bin\")\n",
        "base_files = glob.glob(base_pattern)\n",
        "delta_pattern = os.path.join(tmp_delta_path, \"pytorch_model-*.bin\")\n",
        "delta_files = glob.glob(delta_pattern)\n",
        "delta_state_dict = torch.load(delta_files[0])\n",
        "\n",
        "print(\"Applying the delta\")\n",
        "weight_map = {}\n",
        "total_size = 0\n",
        "\n",
        "# # Fix the order of files. Even though the file name (base_file) not monotonic,\n",
        "# # I can order tham by the position in the list (i)\n",
        "# base_files = sorted(base_files)\n",
        "delta_file = delta_files[0]\n",
        "\n",
        "for i, base_file in tqdm(enumerate(base_files)):\n",
        "\n",
        "    # # Break in the middle to delete some files and save spae\n",
        "    # if i >= 20:\n",
        "    #     break\n",
        "    print(f\"\\nProcess file {i}: {base_file}\")\n",
        "\n",
        "    state_dict = torch.load(base_file)\n",
        "    file_name = f\"pytorch_model-{i:02}.bin\"\n",
        "    for name, param in state_dict.items():\n",
        "        if name not in delta_state_dict:\n",
        "            for delta_file in delta_files:\n",
        "                delta_state_dict = torch.load(delta_file)\n",
        "                gc.collect()\n",
        "                if name in delta_state_dict:\n",
        "                    break\n",
        "        if \"embed_tokens\" in name or \"lm_head.weight\" in name or \"self_attn.rotary_emb.inv_freq\" in name:\n",
        "            state_dict[name] = delta_state_dict[name]\n",
        "        else:\n",
        "            # A terrible workaround in case the sizes of base and delta don't match\n",
        "            delta_weights = delta_state_dict[name]\n",
        "            if state_dict[name].size() != delta_weights.size():\n",
        "                print(f'''\\nSize of {name} in {base_file}: {state_dict[name].size()}\n",
        "                    and in {delta_file}: {delta_weights.size()}''')\n",
        "                delta_weights = delta_weights.\\\n",
        "                    narrow(0, 0, state_dict[name].shape[0]).narrow(1, 0, state_dict[name].shape[1])\n",
        "\n",
        "            state_dict[name] += delta_weights\n",
        "        weight_map[name] = file_name\n",
        "        total_size += param.numel() * param.element_size()\n",
        "        gc.collect()\n",
        "    torch.save(state_dict, os.path.join(target_model_path, file_name))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "V9xlJeKYtlLD",
        "outputId": "3eb42beb-abb9-488f-971b-c143497f47d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying the delta\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Process file 0: /content/tmp/base/pytorch_model-00006-of-00033.bin\n",
            "\n",
            "Size of model.layers.5.self_attn.q_proj.weight in /content/tmp/base/pytorch_model-00006-of-00033.bin: torch.Size([4096, 4096])\n",
            "                    and in /content/tmp/delta/pytorch_model-00004-of-00028.bin: torch.Size([5120, 5120])\n",
            "\n",
            "Size of model.layers.5.self_attn.k_proj.weight in /content/tmp/base/pytorch_model-00006-of-00033.bin: torch.Size([4096, 4096])\n",
            "                    and in /content/tmp/delta/pytorch_model-00004-of-00028.bin: torch.Size([5120, 5120])\n",
            "\n",
            "Size of model.layers.5.self_attn.v_proj.weight in /content/tmp/base/pytorch_model-00006-of-00033.bin: torch.Size([4096, 4096])\n",
            "                    and in /content/tmp/delta/pytorch_model-00004-of-00028.bin: torch.Size([5120, 5120])\n",
            "\n",
            "Size of model.layers.5.self_attn.o_proj.weight in /content/tmp/base/pytorch_model-00006-of-00033.bin: torch.Size([4096, 4096])\n",
            "                    and in /content/tmp/delta/pytorch_model-00004-of-00028.bin: torch.Size([5120, 5120])\n",
            "\n",
            "Size of model.layers.5.mlp.gate_proj.weight in /content/tmp/base/pytorch_model-00006-of-00033.bin: torch.Size([11008, 4096])\n",
            "                    and in /content/tmp/delta/pytorch_model-00005-of-00028.bin: torch.Size([13824, 5120])\n",
            "\n",
            "Size of model.layers.5.mlp.down_proj.weight in /content/tmp/base/pytorch_model-00006-of-00033.bin: torch.Size([4096, 11008])\n",
            "                    and in /content/tmp/delta/pytorch_model-00005-of-00028.bin: torch.Size([5120, 13824])\n",
            "\n",
            "Size of model.layers.5.mlp.up_proj.weight in /content/tmp/base/pytorch_model-00006-of-00033.bin: torch.Size([11008, 4096])\n",
            "                    and in /content/tmp/delta/pytorch_model-00005-of-00028.bin: torch.Size([13824, 5120])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [04:29, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Size of model.layers.5.input_layernorm.weight in /content/tmp/base/pytorch_model-00006-of-00033.bin: torch.Size([4096])\n",
            "                    and in /content/tmp/delta/pytorch_model-00005-of-00028.bin: torch.Size([5120])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92m<cell line: 16>\u001b[0m:\u001b[94m41\u001b[0m                                                                            \u001b[31m│\u001b[0m\n",
              "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
              "\u001b[1;91mIndexError: \u001b[0mtuple index out of range\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 16&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">41</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">IndexError: </span>tuple index out of range\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\n",
        "    os.path.join(target_model_path, \"pytorch_model.bin.index.json\"), \"w\"\n",
        ") as f:\n",
        "    json.dump(\n",
        "        {\"weight_map\": weight_map, \"metadata\": {\"total_size\": total_size}}, f\n",
        "    )\n",
        "\n",
        "print(f\"Saving the target model to {target_model_path}\")\n",
        "delta_tokenizer.save_pretrained(target_model_path)\n",
        "delta_config.save_pretrained(target_model_path)"
      ],
      "metadata": {
        "id": "povE2NlnoCnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Debug"
      ],
      "metadata": {
        "id": "EKrDEpI6oEF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!ls /root/.cache/huggingface/hub/models--decapoda-research--llama-13b-hf/snapshots/438770a656712a5072229b62256521845d4de5ce\n",
        "# !ls -l tmp/base/\n",
        "# !rm -rf /root/.cache/huggingface/hub/models--decapoda-research--llama-13b-hf/snapshots/438770a656712a5072229b62256521845d4de5ce/pytorch_model-0002*\n",
        "# !ls -l /root/.cache/huggingface/hub/\n",
        "# !rm -fr /root/.cache/huggingface/hub/models--decapoda-research--llama-7b-hf/\n",
        "# !ls -l tmp/delta/\n",
        "\n",
        "# !rm -f tmp/base/pytorch_model-0002*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmUfaIvbD1gn",
        "outputId": "c933c03d-5067-4d8d-b097-e79ded0df4c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '/root/.cache/huggingface/hub/models--decapoda-research--llama-13b-hf/snapshots/438770a656712a5072229b62256521845d4de5ce': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WRpeK32qwoLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(name)\n",
        "print(delta_file)\n",
        "delta_state_dict.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYhKpB2xzE_d",
        "outputId": "58162caf-87ad-4105-e450-e10fb179fac3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model.layers.19.self_attn.q_proj.weight\n",
            "/content/tmp/delta/pytorch_model-25.bin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['model.layers.18.mlp.up_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.rotary_emb.inv_freq'])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QktqWdzc0GKC",
        "outputId": "40ca4f7a-8ac9-41b2-92d0-91828a133ffd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.19.self_attn.rotary_emb.inv_freq'])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tmp = torch.load('/content/tmp/delta/pytorch_model-24.bin')\n",
        "tmp.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZA4P2ST60uS_",
        "outputId": "0e9ec753-59bc-488e-8e82-cbf6e2d350ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['model.layers.33.mlp.up_proj.weight', 'model.layers.33.input_layernorm.weight', 'model.layers.33.post_attention_layernorm.weight', 'model.layers.34.self_attn.q_proj.weight', 'model.layers.34.self_attn.k_proj.weight', 'model.layers.34.self_attn.v_proj.weight', 'model.layers.34.self_attn.o_proj.weight', 'model.layers.34.self_attn.rotary_emb.inv_freq', 'model.layers.34.mlp.gate_proj.weight', 'model.layers.34.mlp.down_proj.weight', 'model.layers.34.mlp.up_proj.weight', 'model.layers.34.input_layernorm.weight', 'model.layers.34.post_attention_layernorm.weight', 'model.layers.35.self_attn.q_proj.weight', 'model.layers.35.self_attn.k_proj.weight', 'model.layers.35.self_attn.v_proj.weight', 'model.layers.35.self_attn.o_proj.weight', 'model.layers.35.self_attn.rotary_emb.inv_freq'])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_path\n",
        "# !ls -l tmp/delta\n",
        "!ls -l /root/.cache/huggingface/hub/models--decapoda-research--llama-7b-hf/blobs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WH3Jc3qq010g",
        "outputId": "033f2236-5e33-4707-ba67-424132a6704d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 13161796\n",
            "-rw-r--r-- 1 root root 404770755 Jul 11 01:35 0d708f41d25ad5461d979337713757fabfe76c375b205b536bfe14d48556d5e0\n",
            "-rw-r--r-- 1 root root       141 Jul 11 01:37 1aabb918c81c56bcb61ba76e6f93dc3ff601ee1a\n",
            "-rw-r--r-- 1 root root 404770755 Jul 11 01:36 1b35993de04e54aad4a034da33ae2860220482c0be5f5c2b9555c13f969f6531\n",
            "-rw-r--r-- 1 root root 404770755 Jul 11 01:37 2aef4f0698b0670d6e9c83ca103c3c8dff70e2f237065e2c7293f496933ead0e\n",
            "-rw-r--r-- 1 root root 404770755 Jul 11 01:36 33d306be39a567f349b9e7bdc8b2543158607ca8834d8c71d32e1b648367ca94\n",
            "-rw-r--r-- 1 root root 404770755 Jul 11 01:35 3cfe1d1093cbbb04d36b4fd09978e87077e41b4492b8cbba6bfff57c8dbd86dd\n",
            "-rw-r--r-- 1 root root 404770755 Jul 11 01:37 3e746351f91d9d482c534620a441a7a66474fa9da57104b65f821fc513bad663\n",
            "-rw-r--r-- 1 root root       427 Jul 11 01:34 437be862ab867ceb4b1a6e8d3c1187ade70c22c3\n",
            "-rw-r--r-- 1 root root 404770755 Jul 11 01:35 6080c736b3136935f0d3fb9fc65b41008c65b6b7d5238e1b9c4780043f31b955\n",
            "-rw-r--r-- 1 root root 404770755 Jul 11 01:37 798176e2a686e6ae5e9532d1943baaee618190f81f52311610c91603c6408fa7\n",
            "-rw-r--r-- 1 root root     25477 Jul 11 01:37 8259feb8c524999a31b6351b85d6c1df3222466d\n",
            "-rw-r--r-- 1 root root 404770755 Jul 11 01:35 869ac6cceca6c9f26d13ac05549b5e15d79acb90a0593475260171c29f3b9c73\n",
            "-rw-r--r-- 1 root root 404770755 Jul 11 01:35 8d551e0c80f82a61a1cbf74389651359c3fb35ce50c4568256ebc8dcf7c1fcd3\n",
            "-rw-r--r-- 1 root root 404770755 Jul 11 01:35 90047b0adfef1abc939032be63cbe62300d3ee18b39251c2f36d362d6735b64e\n",
            "-rw-r--r-- 1 root root 404770755 Jul 11 01:37 95a865bd2b40a83e2110bb0a7de97dd08d7852b3858538ec5d65f64084667e44\n",
            "-rw-r--r-- 1 root root 404770755 Jul 11 01:35 98ce0f7e5e8c374e697ded9576dfc5023f83e3c4c8a65dce63c009401672d2ca\n",
            "-rw-r--r-- 1 root root 404770755 Jul 11 01:37 9c06dbeda998897337596e8d98d53be174083a09a049640bf4f31eef37f56ea0\n",
            "-rw-r--r-- 1 root root 404770755 Jul 11 01:36 9e13fe2d35345fe65a1f7d4af98cf9a40f758f2403bd68a7e8ca4dbf50598014\n",
            "-rw-r--r-- 1 root root         2 Jul 11 01:37 9e26dfeeb6e641a33dae4961196235bdb965b21b\n",
            "-rw-r--r-- 1 root root    499723 Jul 11 01:37 9e556afd44213b6bd1be2b850ebbbd98f5481437a8021afaf58ee7fb1818d347\n",
            "-rw-r--r-- 1 root root 404770755 Jul 11 01:35 9e6b78baebf20d60da825c3654c202008acfc3056c84cb6dadd91b155d278e38\n",
            "-rw-r--r-- 1 root root 404770755 Jul 11 01:35 a18845375f10dc0085adc387f21e6ddc372338e767d1879c6369c11b27cb9b97\n",
            "-rw-r--r-- 1 root root 404770755 Jul 11 01:36 ada30914f430170e601b4638ade39cbebb2cab280763ad5310492a35fed05082\n",
            "-rw-r--r-- 1 root root     10646 Jul 11 01:34 b1c9239ba64af3ac330790fbdc8a2282612bf437\n",
            "-rw-r--r-- 1 root root 404770755 Jul 11 01:35 bba688e4ab77a5a95aebf52e0aa508d31ec1c02b6b5078191379c158526e6c12\n",
            "-rw-r--r-- 1 root root 404770755 Jul 11 01:35 c259f397f43ccb73919b40810c4b33638bc655301a80805374800c124216ec80\n",
            "-rw-r--r-- 1 root root 404770755 Jul 11 01:36 c3cb8f788c63a46e026ae54fca16aeb57d0af45e22bed01124dee12c266cf00d\n",
            "-rw-r--r-- 1 root root 404770755 Jul 11 01:35 c5cb2ef8a6558c0ce314bcc11e4a15d0df379222ad7753ae9f75591cc87c945d\n",
            "-rw-r--r-- 1 root root      1477 Jul 11 01:34 c7d9f3332a950355d5a77d85000f05e6f45435ea\n",
            "-rw-r--r-- 1 root root      8312 Jul 11 01:34 c8c957df3d12381e3f5dacbfe08c76799b374297\n",
            "-rw-r--r-- 1 root root 404770755 Jul 11 01:35 c8e5dbe9332ea27857a2f9b8ef80e34567ff5d6f5855daffd73292080aedb401\n",
            "-rw-r--r-- 1 root root 404770755 Jul 11 01:36 c993cd7044316208e3b8d8564d929e95bb57328dab63848fbb729b10049861eb\n",
            "-rw-r--r-- 1 root root 404770755 Jul 11 01:36 cb454682956e64a8c7c035764a6110dbb76fd7c61285160b42b06e7f0b5db0f2\n",
            "-rw-r--r-- 1 root root 404770755 Jul 11 01:35 cc8014666b4da29ee637d96030f5e7612d2f2dff79a4df8ad4aafcb0125357d3\n",
            "-rw-r--r-- 1 root root 404770755 Jul 11 01:36 cd034aad548c5423a8b4e07a4c44d8c2be1c350f9e8681cfdf0e63328fabadf9\n",
            "-rw-r--r-- 1 root root 404770755 Jul 11 01:37 ce7a9ef1002e65ff614d8237d6a643b4b6b96ef68b260532f219c692884f9b70\n",
            "-rw-r--r-- 1 root root 404770755 Jul 11 01:37 d6b38acac958aadb83ae22e7317a82ff64caccde6f0c02539bd499e8777218af\n",
            "-rw-r--r-- 1 root root 404770755 Jul 11 01:37 d794d97063fb3c24ce913efa473b393dc6330aa67b00a47c4a91428de03896bc\n",
            "-rw-r--r-- 1 root root 404770755 Jul 11 01:35 ec40a890dc20ddf9b49a6e1b18e26a304adbe701374a92f0295a7975139fa4b3\n",
            "-rw-r--r-- 1 root root 524297676 Jul 11 01:37 f289012ffe06b1aee0bebdfaea70a394c60444370d9b31c9ab0a7ccd7a4ee415\n",
            "-rw-r--r-- 1 root root 404770755 Jul 11 01:36 f30a61cf2d7bef7ea0b927e05cd0f43d0aee6e00de73a9f7ba56f99deb4e3905\n",
            "-rw-r--r-- 1 root root       124 Jul 11 01:34 fcac46fde0bfa2d864ab81547a4bf2d9f0737ccf\n"
          ]
        }
      ]
    }
  ]
}