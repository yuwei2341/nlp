{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "# from nltk.cluster import KMeansClusterer, euclidean_distance\n",
    "# from gensim import corpora, models, utils\n",
    "from numpy import array\n",
    "from gensim import corpora, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",              \n",
    "             \"Relation of user perceived response time to error measurement\",\n",
    "             \"The generation of random binary unordered trees\",\n",
    "             \"The intersection graph of paths in trees\",\n",
    "             \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "             \"Graph minors A survey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_file = 'tmp.txt'\n",
    "with open(doc_file, 'w') as f:\n",
    "    for ii in documents:\n",
    "        f.write(\"%s\\n\" % ii)\n",
    "    \n",
    "with open(doc_file, 'r') as f:\n",
    "    adocuments = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_doc(document):\n",
    "    '''Parse a single doc\n",
    "    removing whitespaces, punctuations, stopwords, and stemming words\n",
    "    '''\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    stop = stopwords.words('english')\n",
    "    stemmer = LancasterStemmer()\n",
    "\n",
    "    # Remove non-alphanumeric character\n",
    "    intermediate = tokenizer.tokenize(document)    \n",
    "\n",
    "    # Take lower case\n",
    "    intermediate = [i.lower() for i in intermediate]\n",
    "\n",
    "    # Remove stop words\n",
    "    intermediate = [i for i in intermediate if i not in stop]\n",
    "\n",
    "    # Get stems        \n",
    "    parsed = [stemmer.stem(i) for i in intermediate]\n",
    "\n",
    "    return parsed\n",
    "    \n",
    "    \n",
    "class IndexCreator(object):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, doc_file=None, docs=None):\n",
    "        '''\n",
    "        Take input, either docs file, or existing Index\n",
    "        '''\n",
    "\n",
    "        self.doc_file = doc_file\n",
    "        self.documents = docs\n",
    "        self.processed = None\n",
    "        self._word_index = defaultdict(lambda: defaultdict(list))\n",
    "        \n",
    "    def _read_doc_file(self):\n",
    "        '''read doc file if not None'''\n",
    "        \n",
    "        if self.doc_file is not None:\n",
    "            with open(self.doc_file, 'r') as f:\n",
    "                documents = f.readlines()\n",
    "        \n",
    "            if self.documents is not None:\n",
    "                print \"Overwrite document with that read from the file\"\n",
    "            \n",
    "            self.documents = documents\n",
    "        \n",
    "    def _parse_collection(self):\n",
    "        # parse all docs  \n",
    "        processed = []\n",
    "        for document in self.documents:    \n",
    "            processed.append(parse_doc(document))\n",
    "        self.processed = processed\n",
    "        \n",
    "        \n",
    "    def _build_index(self):    \n",
    "        '''Build inverted index'''\n",
    "\n",
    "        for i, doc in enumerate(self.processed):\n",
    "            for j, word in enumerate(doc):\n",
    "                self._word_index[word][i].append(j)\n",
    "                \n",
    "    def create_index(self):\n",
    "        '''Master function to create inex'''\n",
    "        \n",
    "        self._read_doc_file()\n",
    "        if self.documents is None:\n",
    "            print \"Failed to read documents\"\n",
    "            return\n",
    "        self._parse_collection()\n",
    "        self._build_index()\n",
    "        \n",
    "    @property\n",
    "    def word_index(self):\n",
    "        return dict(self._word_index)\n",
    "    \n",
    "    def dump_index(self, file_name='word_index.p'):\n",
    "        with open(file_name, 'w') as f:\n",
    "            pickle.dump((dict(self._word_index), self.documents), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query by Words, rank by docs containing most words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Query = 'system human'\n",
    "parsed = word_parser(Query)\n",
    "\n",
    "class WordQuerier(object):\n",
    "    '''\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, index_file=None, word_index=None, documents=None):\n",
    "        '''\n",
    "        '''\n",
    "        \n",
    "        self.index_file = index_file\n",
    "        self.word_index = word_index\n",
    "        self.documents = documents\n",
    "        self.doc_freq = None\n",
    "        \n",
    "        self._read_index_file()\n",
    "        \n",
    "    def _read_index_file(self):\n",
    "        '''read index file if not None'''\n",
    "        \n",
    "        if self.index_file is not None:\n",
    "            with open(self.index_file, 'r') as f:\n",
    "                word_index, documents = pickle.load(f)\n",
    "        \n",
    "            if self.word_index is not None:\n",
    "                print \"Overwrite word index with that read from the file\"\n",
    "            \n",
    "            self.word_index = word_index\n",
    "            self.documents = documents\n",
    "            \n",
    "    def query_words(self, words):\n",
    "\n",
    "        parsed = parse_doc(words)    \n",
    "\n",
    "        self.doc_freq = defaultdict(int)\n",
    "        for word in parsed:        \n",
    "            if word not in self.word_index:\n",
    "                continue\n",
    "            word_dict_value = self.word_index[word].items()\n",
    "            ind = word_dict_value\n",
    "\n",
    "            # Get doc and positions of word in doc\n",
    "            # Below turns out faster than: ind = sorted(tmp.items(), key=lambda x: len(operator.itemgetter(1)(x)), reverse=True)\n",
    "            # ind = sorted(word_dict_value, key=lambda x: len(x[1]), reverse=True)\n",
    "            \n",
    "            # Get doc frequency for all words\n",
    "            for i in ind:\n",
    "                self.doc_freq[i[0]] += 1\n",
    "\n",
    "        ind_sorted = sorted(self.doc_freq.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        return [self.documents[i[0]] for i in ind_sorted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwrite document with that read from the file\n"
     ]
    }
   ],
   "source": [
    "a = IndexCreator(doc_file='tmp.txt', docs=adocuments)\n",
    "a.create_index()\n",
    "a.dump_index('tmp.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = 'system human search'\n",
    "b = WordQuerier(index_file='tmp.p')\n",
    "c = b.query_words(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank by similarity of tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get idf of each term in corpus, and tf for each document\n",
    "1. For a new query, get all terms in it\n",
    "1. If there is no duplicate terms, the tfidf vector of the query is proportional to the idf for all terms in it\n",
    "1. Compute the TFIDF vector for each doc only using terms in the query\n",
    "1. Compute similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-28 11:13:47,055 : INFO : collecting document frequencies\n",
      "2017-07-28 11:13:47,057 : INFO : PROGRESS: processing document #0\n",
      "2017-07-28 11:13:47,061 : INFO : calculating IDF weights for 9 documents and 11 features (28 matrix non-zeros)\n"
     ]
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5773502691896257), (1, 0.5773502691896257), (2, 0.5773502691896257)]\n",
      "[(1, 0.44424552527467476), (3, 0.44424552527467476), (4, 0.44424552527467476), (5, 0.44424552527467476), (6, 0.3244870206138555), (7, 0.3244870206138555)]\n",
      "[(0, 0.5710059809418182), (6, 0.4170757362022777), (7, 0.4170757362022777), (8, 0.5710059809418182)]\n",
      "[(2, 0.49182558987264147), (6, 0.7184811607083769), (8, 0.49182558987264147)]\n",
      "[(3, 0.6282580468670046), (4, 0.6282580468670046), (7, 0.45889394536615247)]\n",
      "[(9, 1.0)]\n",
      "[(9, 0.7071067811865475), (10, 0.7071067811865475)]\n",
      "[(9, 0.5080429008916749), (10, 0.5080429008916749), (11, 0.695546419520037)]\n",
      "[(5, 0.6282580468670046), (10, 0.45889394536615247), (11, 0.6282580468670046)]\n"
     ]
    }
   ],
   "source": [
    "corpus_tfidf = tfidf[corpus]\n",
    "for i in corpus_tfidf:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Similarity\n",
    "https://radimrehurek.com/gensim/tut3.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-28 11:47:01,660 : INFO : loading Dictionary object from /var/folders/pv/r00j5j0n5g99lmw66v7rhlqc0000gn/T/deerwester.dict\n",
      "2017-07-28 11:47:01,662 : INFO : loaded /var/folders/pv/r00j5j0n5g99lmw66v7rhlqc0000gn/T/deerwester.dict\n",
      "2017-07-28 11:47:01,664 : INFO : loaded corpus index from /var/folders/pv/r00j5j0n5g99lmw66v7rhlqc0000gn/T/deerwester.mm.index\n",
      "2017-07-28 11:47:01,666 : INFO : initializing corpus reader from /var/folders/pv/r00j5j0n5g99lmw66v7rhlqc0000gn/T/deerwester.mm\n",
      "2017-07-28 11:47:01,668 : INFO : accepted corpus with 9 documents, 12 features, 28 non-zero entries\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary.load(os.path.join(TEMP_FOLDER, 'deerwester.dict'))\n",
    "corpus = corpora.MmCorpus(os.path.join(TEMP_FOLDER, 'deerwester.mm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-28 11:47:21,126 : INFO : using serial LSI version on this node\n",
      "2017-07-28 11:47:21,128 : INFO : updating model with new documents\n",
      "2017-07-28 11:47:21,130 : INFO : preparing a new chunk of documents\n",
      "2017-07-28 11:47:21,132 : INFO : using 100 extra samples and 2 power iterations\n",
      "2017-07-28 11:47:21,133 : INFO : 1st phase: constructing (12, 102) action matrix\n",
      "2017-07-28 11:47:21,134 : INFO : orthonormalizing (12, 102) action matrix\n",
      "2017-07-28 11:47:21,137 : INFO : 2nd phase: running dense svd on (12, 9) matrix\n",
      "2017-07-28 11:47:21,139 : INFO : computing the final decomposition\n",
      "2017-07-28 11:47:21,141 : INFO : keeping 2 factors (discarding 43.156% of energy spectrum)\n",
      "2017-07-28 11:47:21,142 : INFO : processed documents up to #9\n",
      "2017-07-28 11:47:21,144 : INFO : topic #0(3.341): 0.644*\"system\" + 0.404*\"user\" + 0.301*\"eps\" + 0.265*\"time\" + 0.265*\"response\" + 0.240*\"computer\" + 0.221*\"human\" + 0.206*\"survey\" + 0.198*\"interface\" + 0.036*\"graph\"\n",
      "2017-07-28 11:47:21,145 : INFO : topic #1(2.542): -0.623*\"graph\" + -0.490*\"trees\" + -0.451*\"minors\" + -0.274*\"survey\" + 0.167*\"system\" + 0.141*\"eps\" + 0.113*\"human\" + -0.107*\"response\" + -0.107*\"time\" + 0.072*\"interface\"\n"
     ]
    }
   ],
   "source": [
    "lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.46182100453271624), (1, 0.070027665279001478)] [(1, 1), (2, 1)]\n"
     ]
    }
   ],
   "source": [
    "doc = \"Human computer interaction\"\n",
    "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "vec_lsi = lsi[vec_bow]\n",
    "print vec_lsi, vec_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-28 12:00:03,454 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "2017-07-28 12:00:03,458 : INFO : creating matrix with 9 documents and 2 features\n"
     ]
    }
   ],
   "source": [
    "index = similarities.MatrixSimilarity(lsi[corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-28 12:00:50,739 : INFO : saving MatrixSimilarity object under /var/folders/pv/r00j5j0n5g99lmw66v7rhlqc0000gn/T/deerwester.index, separately None\n",
      "2017-07-28 12:00:50,742 : INFO : saved /var/folders/pv/r00j5j0n5g99lmw66v7rhlqc0000gn/T/deerwester.index\n",
      "2017-07-28 12:00:50,744 : INFO : loading MatrixSimilarity object from /var/folders/pv/r00j5j0n5g99lmw66v7rhlqc0000gn/T/deerwester.index\n",
      "2017-07-28 12:00:50,746 : INFO : loaded /var/folders/pv/r00j5j0n5g99lmw66v7rhlqc0000gn/T/deerwester.index\n"
     ]
    }
   ],
   "source": [
    "index.save(os.path.join(TEMP_FOLDER, 'deerwester.index'))\n",
    "index = similarities.MatrixSimilarity.load(os.path.join(TEMP_FOLDER, 'deerwester.index'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.99809301), (1, 0.93748635), (2, 0.99844527), (3, 0.9865886), (4, 0.90755945), (5, -0.12416792), (6, -0.10639259), (7, -0.098794632), (8, 0.050041769)]\n"
     ]
    }
   ],
   "source": [
    "sims = index[vec_lsi]\n",
    "print list(enumerate(sims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
