{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## todo later\n",
    "1. remove unicode stuff\n",
    "1. split newline in a more general way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
      "\r",
      "\u000b",
      "\f",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string, re\n",
    "print string.printable\n",
    "\n",
    "re.sub('[\\W_]+', '', string.printable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import glob\n",
    "import sys\n",
    "import random\n",
    "import collections\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "class TextGenerator(object):\n",
    "    '''\n",
    "    '''\n",
    "\n",
    "    EMPTY_STRING = ''\n",
    "    CUTOFF = 100\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''\n",
    "        '''\n",
    "        \n",
    "        self.documents_raw = []\n",
    "        self.mc = collections.defaultdict(list)\n",
    "        self.paragraphs = []\n",
    "        \n",
    "    def _get_params(self):\n",
    "        param = sys.argv\n",
    "        self.input_directory_name = param[1]\n",
    "        self.P = param[2]\n",
    "        self.N = param[3]\n",
    "\n",
    "    def _read_file(self):\n",
    "        files = glob.glob(os.path.join(self.input_directory_name, '*'))\n",
    "        for i in files:\n",
    "            with open(i, 'r') as f:\n",
    "                self.documents_raw.append(f.read())\n",
    "    \n",
    "    @classmethod\n",
    "    def _parse_doc(document):\n",
    "        '''Parse a single paragraph\n",
    "        Remove whitespaces, punctuations    \n",
    "        '''\n",
    "\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        intermediate = tokenizer.tokenize(document)    \n",
    "        parsed = [i.lower() for i in intermediate]\n",
    "\n",
    "        return parsed\n",
    "\n",
    "    def _parse_collection(self):\n",
    "        # parse all docs  \n",
    "        \n",
    "        self.documents_raw\n",
    "        for raw_text in self.documents_raw:\n",
    "            doc = re.sub('[\\W_]+', '', unicode(raw_text, 'ascii', 'ignore'))\n",
    "            paragraphs = .split('\\r\\n\\r\\n')       \n",
    "            for paragraph in paragraphs:    \n",
    "                parsed = self._parse_doc(document)\n",
    "                if parsed:\n",
    "                    self.processed.append(parsed)\n",
    "        \n",
    "    def _process_input(self):\n",
    "\n",
    "        key_start = [self.EMPTY_STRING] * self.P\n",
    "\n",
    "        for line in processed:\n",
    "            key = key_start\n",
    "            for word in line:\n",
    "                self.mc[tuple(key)].append(word)\n",
    "                key = key[1:] + [word]\n",
    "            self.mc[tuple(key)].append(EMPTY_STRING)\n",
    "\n",
    "    def _generate_output(self):\n",
    "        \n",
    "        key_start = [self.EMPTY_STRING] * self.P\n",
    "        self.paragraphs = []\n",
    "\n",
    "        j = 0\n",
    "        while j < self.N:\n",
    "            key = key_start\n",
    "            paragraph = []\n",
    "            for i in range(self.CUTOFF):\n",
    "                suf_start = self.mc[tuple(key)]\n",
    "                next_word = random.choice(suf_start)\n",
    "                if next_word is self.EMPTY_STRING:\n",
    "                    break\n",
    "                paragraph.append(next_word)\n",
    "                key = key[1:] + [next_word]\n",
    "            joined = ' '.join(paragraph)\n",
    "            if joined:\n",
    "                self.paragraphs.append(joined)\n",
    "                j += 1\n",
    "    \n",
    "    def _print_result(self):\n",
    "        for i in self.paragraphs:\n",
    "            print i + '\\n'\n",
    "        print 'Number of prefixes: {}'.format(len(self.mc))\n",
    "        \n",
    "    def run(self):\n",
    "        \n",
    "        self._get_params()\n",
    "        self._read_file()\n",
    "        self._parse_collection()\n",
    "        self._process_input()\n",
    "        self._generate_output()\n",
    "        self._print_result()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents_raw = []\n",
    "files = glob.glob(os.path.join('/Users/yuwei/Tools/nlp/topics/data', '*'))\n",
    "for i in files:\n",
    "    with open(i, 'r') as f:\n",
    "        documents_raw.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processed = []\n",
    "raw_text = documents_raw[0]\n",
    "doc = pattern.sub('', unicode(raw_text, 'ascii', 'ignore').lower())\n",
    "paragraphs = doc.split('\\r\\n\\r\\n')        \n",
    "for paragraph in paragraphs:    \n",
    "    parsed = paragraph.split()\n",
    "    if parsed:\n",
    "        processed.append(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'the project gutenberg ebook of pride and prejudice by jane austen',\n",
       " u'this ebook is for the use of anyone anywhere at no cost and with\\r\\nalmost no restrictions whatsoever  you may copy it give it away or\\r\\nreuse it under the terms of the project gutenberg license included\\r\\nwith this ebook or online at wwwgutenbergorg',\n",
       " u'\\r\\ntitle pride and prejudice',\n",
       " u'author jane austen',\n",
       " u'posting date august 26 2008 ebook 1342\\r\\nrelease date june 1998\\r\\nlast updated march 10 2018',\n",
       " u'language english',\n",
       " u'character set encoding utf8',\n",
       " u' start of this project gutenberg ebook pride and prejudice ',\n",
       " u'',\n",
       " u'\\r\\nproduced by anonymous volunteers']"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Parse the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Query by Words, rank by docs containing most words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Query = 'system human'\n",
    "parsed = word_parser(Query)\n",
    "\n",
    "class WordQuerier(object):\n",
    "    '''\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, index_file=None, word_index=None, documents=None):\n",
    "        '''\n",
    "        '''\n",
    "        \n",
    "        self.index_file = index_file\n",
    "        self.word_index = word_index\n",
    "        self.documents = documents\n",
    "        self.doc_freq = None\n",
    "        \n",
    "        self._read_index_file()\n",
    "        \n",
    "    def _read_index_file(self):\n",
    "        '''read index file if not None'''\n",
    "        \n",
    "        if self.index_file is not None:\n",
    "            with open(self.index_file, 'r') as f:\n",
    "                word_index, documents = pickle.load(f)\n",
    "        \n",
    "            if self.word_index is not None:\n",
    "                print \"Overwrite word index with that read from the file\"\n",
    "            \n",
    "            self.word_index = word_index\n",
    "            self.documents = documents\n",
    "            \n",
    "    def query_words(self, words):\n",
    "\n",
    "        parsed = parse_doc(words)    \n",
    "\n",
    "        self.doc_freq = defaultdict(int)\n",
    "        for word in parsed:        \n",
    "            if word not in self.word_index:\n",
    "                continue\n",
    "            word_dict_value = self.word_index[word].items()\n",
    "            ind = word_dict_value\n",
    "\n",
    "            # Get doc and positions of word in doc\n",
    "            # Below turns out faster than: ind = sorted(tmp.items(), key=lambda x: len(operator.itemgetter(1)(x)), reverse=True)\n",
    "            # ind = sorted(word_dict_value, key=lambda x: len(x[1]), reverse=True)\n",
    "            \n",
    "            # Get doc frequency for all words\n",
    "            for i in ind:\n",
    "                self.doc_freq[i[0]] += 1\n",
    "\n",
    "        ind_sorted = sorted(self.doc_freq.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        return [self.documents[i[0]] for i in ind_sorted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwrite document with that read from the file\n"
     ]
    }
   ],
   "source": [
    "a = IndexCreator(doc_file='tmp.txt', docs=adocuments)\n",
    "a.create_index()\n",
    "a.dump_index('tmp.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "words = 'system human search'\n",
    "b = WordQuerier(index_file='tmp.p')\n",
    "c = b.query_words(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Rank by similarity of tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "1. Get idf of each term in corpus, and tf for each document\n",
    "1. For a new query, get all terms in it\n",
    "1. If there is no duplicate terms, the tfidf vector of the query is proportional to the idf for all terms in it\n",
    "1. Compute the TFIDF vector for each doc only using terms in the query\n",
    "1. Compute similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-28 11:13:47,055 : INFO : collecting document frequencies\n",
      "2017-07-28 11:13:47,057 : INFO : PROGRESS: processing document #0\n",
      "2017-07-28 11:13:47,061 : INFO : calculating IDF weights for 9 documents and 11 features (28 matrix non-zeros)\n"
     ]
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5773502691896257), (1, 0.5773502691896257), (2, 0.5773502691896257)]\n",
      "[(1, 0.44424552527467476), (3, 0.44424552527467476), (4, 0.44424552527467476), (5, 0.44424552527467476), (6, 0.3244870206138555), (7, 0.3244870206138555)]\n",
      "[(0, 0.5710059809418182), (6, 0.4170757362022777), (7, 0.4170757362022777), (8, 0.5710059809418182)]\n",
      "[(2, 0.49182558987264147), (6, 0.7184811607083769), (8, 0.49182558987264147)]\n",
      "[(3, 0.6282580468670046), (4, 0.6282580468670046), (7, 0.45889394536615247)]\n",
      "[(9, 1.0)]\n",
      "[(9, 0.7071067811865475), (10, 0.7071067811865475)]\n",
      "[(9, 0.5080429008916749), (10, 0.5080429008916749), (11, 0.695546419520037)]\n",
      "[(5, 0.6282580468670046), (10, 0.45889394536615247), (11, 0.6282580468670046)]\n"
     ]
    }
   ],
   "source": [
    "corpus_tfidf = tfidf[corpus]\n",
    "for i in corpus_tfidf:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Similarity\n",
    "https://radimrehurek.com/gensim/tut3.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-28 11:47:01,660 : INFO : loading Dictionary object from /var/folders/pv/r00j5j0n5g99lmw66v7rhlqc0000gn/T/deerwester.dict\n",
      "2017-07-28 11:47:01,662 : INFO : loaded /var/folders/pv/r00j5j0n5g99lmw66v7rhlqc0000gn/T/deerwester.dict\n",
      "2017-07-28 11:47:01,664 : INFO : loaded corpus index from /var/folders/pv/r00j5j0n5g99lmw66v7rhlqc0000gn/T/deerwester.mm.index\n",
      "2017-07-28 11:47:01,666 : INFO : initializing corpus reader from /var/folders/pv/r00j5j0n5g99lmw66v7rhlqc0000gn/T/deerwester.mm\n",
      "2017-07-28 11:47:01,668 : INFO : accepted corpus with 9 documents, 12 features, 28 non-zero entries\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary.load(os.path.join(TEMP_FOLDER, 'deerwester.dict'))\n",
    "corpus = corpora.MmCorpus(os.path.join(TEMP_FOLDER, 'deerwester.mm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-28 11:47:21,126 : INFO : using serial LSI version on this node\n",
      "2017-07-28 11:47:21,128 : INFO : updating model with new documents\n",
      "2017-07-28 11:47:21,130 : INFO : preparing a new chunk of documents\n",
      "2017-07-28 11:47:21,132 : INFO : using 100 extra samples and 2 power iterations\n",
      "2017-07-28 11:47:21,133 : INFO : 1st phase: constructing (12, 102) action matrix\n",
      "2017-07-28 11:47:21,134 : INFO : orthonormalizing (12, 102) action matrix\n",
      "2017-07-28 11:47:21,137 : INFO : 2nd phase: running dense svd on (12, 9) matrix\n",
      "2017-07-28 11:47:21,139 : INFO : computing the final decomposition\n",
      "2017-07-28 11:47:21,141 : INFO : keeping 2 factors (discarding 43.156% of energy spectrum)\n",
      "2017-07-28 11:47:21,142 : INFO : processed documents up to #9\n",
      "2017-07-28 11:47:21,144 : INFO : topic #0(3.341): 0.644*\"system\" + 0.404*\"user\" + 0.301*\"eps\" + 0.265*\"time\" + 0.265*\"response\" + 0.240*\"computer\" + 0.221*\"human\" + 0.206*\"survey\" + 0.198*\"interface\" + 0.036*\"graph\"\n",
      "2017-07-28 11:47:21,145 : INFO : topic #1(2.542): -0.623*\"graph\" + -0.490*\"trees\" + -0.451*\"minors\" + -0.274*\"survey\" + 0.167*\"system\" + 0.141*\"eps\" + 0.113*\"human\" + -0.107*\"response\" + -0.107*\"time\" + 0.072*\"interface\"\n"
     ]
    }
   ],
   "source": [
    "lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.46182100453271624), (1, 0.070027665279001478)] [(1, 1), (2, 1)]\n"
     ]
    }
   ],
   "source": [
    "doc = \"Human computer interaction\"\n",
    "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "vec_lsi = lsi[vec_bow]\n",
    "print vec_lsi, vec_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-28 12:00:03,454 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "2017-07-28 12:00:03,458 : INFO : creating matrix with 9 documents and 2 features\n"
     ]
    }
   ],
   "source": [
    "index = similarities.MatrixSimilarity(lsi[corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-28 12:00:50,739 : INFO : saving MatrixSimilarity object under /var/folders/pv/r00j5j0n5g99lmw66v7rhlqc0000gn/T/deerwester.index, separately None\n",
      "2017-07-28 12:00:50,742 : INFO : saved /var/folders/pv/r00j5j0n5g99lmw66v7rhlqc0000gn/T/deerwester.index\n",
      "2017-07-28 12:00:50,744 : INFO : loading MatrixSimilarity object from /var/folders/pv/r00j5j0n5g99lmw66v7rhlqc0000gn/T/deerwester.index\n",
      "2017-07-28 12:00:50,746 : INFO : loaded /var/folders/pv/r00j5j0n5g99lmw66v7rhlqc0000gn/T/deerwester.index\n"
     ]
    }
   ],
   "source": [
    "index.save(os.path.join(TEMP_FOLDER, 'deerwester.index'))\n",
    "index = similarities.MatrixSimilarity.load(os.path.join(TEMP_FOLDER, 'deerwester.index'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.99809301), (1, 0.93748635), (2, 0.99844527), (3, 0.9865886), (4, 0.90755945), (5, -0.12416792), (6, -0.10639259), (7, -0.098794632), (8, 0.050041769)]\n"
     ]
    }
   ],
   "source": [
    "sims = index[vec_lsi]\n",
    "print list(enumerate(sims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
